layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label_img1"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_value: 0.0
    mean_value: 0.0
    mean_value: 0.0
  }
  image_data_param {
    source: "/home/iiau/Desktop/code_cvpr18/stage1/train/data/train_duts.txt"
    batch_size: 1
    new_height: 384
    new_width: 384
  }
}
layer {
  name: "label"
  type: "ImageData"
  top: "label"
  top: "label_gt"
  include {
    phase: TRAIN
  }
  image_data_param {
    source: "/home/iiau/Desktop/code_cvpr18/stage1/train/data/train_duts_gt01.txt"
    is_color: false
    batch_size: 1
    new_height: 384
    new_width: 384
  }
}
layer {
  name: "label_img1"
  type: "Silence"
  bottom: "label_img1"
}
layer {
  name: "label_gt"
  type: "Silence"
  bottom: "label_gt"
}
layer {
  name: "data_bn"
  type: "BatchNorm"
  bottom: "data"
  top: "data_bn"
  param {
    name: "para1-data"
    lr_mult: 0.0
  }
  param {
    name: "para2-data"
    lr_mult: 0.0
  }
  param {
    name: "para3-data"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "data_scale"
  type: "Scale"
  bottom: "data_bn"
  top: "data_bn"
  param {
    name: "lamda-data"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-data"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data_bn"
  top: "conv1"
  param {
    name: "weight-c1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "bias-c1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    pad: 3
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "msra"
      variance_norm: FAN_OUT
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "conv1_bn"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    name: "para1-c1"
    lr_mult: 0.0
  }
  param {
    name: "para2-c1"
    lr_mult: 0.0
  }
  param {
    name: "para3-c1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv1_scale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    name: "lamda-c1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-c1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1_relu"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_pool"
  type: "Pooling"
  bottom: "conv1"
  top: "conv1_pool"
  pooling_param {
    kernel_size: 3
    stride: 2
    pad: 0
  }
}
layer {
  name: "layer_64_1_conv1"
  type: "Convolution"
  bottom: "conv1_pool"
  top: "layer_64_1_conv1"
  param {
    name: "weight-64_1_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_1_bn2"
  type: "BatchNorm"
  bottom: "layer_64_1_conv1"
  top: "layer_64_1_conv1"
  param {
    name: "para1-64_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_1_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_1_scale2"
  type: "Scale"
  bottom: "layer_64_1_conv1"
  top: "layer_64_1_conv1"
  param {
    name: "lamda-64_1_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_1_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_1_relu2"
  type: "ReLU"
  bottom: "layer_64_1_conv1"
  top: "layer_64_1_conv1"
}
layer {
  name: "layer_64_1_conv2"
  type: "Convolution"
  bottom: "layer_64_1_conv1"
  top: "layer_64_1_conv2"
  param {
    name: "weight-64_1_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_1_bn3"
  type: "BatchNorm"
  bottom: "layer_64_1_conv2"
  top: "layer_64_1_conv2"
  param {
    name: "para1-64_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_1_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_1_scale3"
  type: "Scale"
  bottom: "layer_64_1_conv2"
  top: "layer_64_1_conv2"
  param {
    name: "lamda-64_1_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_1_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_1_relu3"
  type: "ReLU"
  bottom: "layer_64_1_conv2"
  top: "layer_64_1_conv2"
}
layer {
  name: "layer_64_1_conv3"
  type: "Convolution"
  bottom: "layer_64_1_conv2"
  top: "layer_64_1_conv3"
  param {
    name: "weight-64_1_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_1_conv_expand"
  type: "Convolution"
  bottom: "layer_64_1_conv1"
  top: "layer_64_1_conv_expand"
  param {
    name: "weight-64_1_conv_expand"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_1_sum"
  type: "Eltwise"
  bottom: "layer_64_1_conv3"
  bottom: "layer_64_1_conv_expand"
  top: "layer_64_1_sum"
}
layer {
  name: "layer_64_2_bn1"
  type: "BatchNorm"
  bottom: "layer_64_1_sum"
  top: "layer_64_2_bn1"
  param {
    name: "para1-64_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_2_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_2_scale1"
  type: "Scale"
  bottom: "layer_64_2_bn1"
  top: "layer_64_2_bn1"
  param {
    name: "lamda-64_2_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_2_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_2_relu1"
  type: "ReLU"
  bottom: "layer_64_2_bn1"
  top: "layer_64_2_bn1"
}
layer {
  name: "layer_64_2_conv1"
  type: "Convolution"
  bottom: "layer_64_2_bn1"
  top: "layer_64_2_conv1"
  param {
    name: "weight-64_2_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_2_bn2"
  type: "BatchNorm"
  bottom: "layer_64_2_conv1"
  top: "layer_64_2_conv1"
  param {
    name: "para1-64_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_2_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_2_scale2"
  type: "Scale"
  bottom: "layer_64_2_conv1"
  top: "layer_64_2_conv1"
  param {
    name: "lamda-64_2_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_2_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_2_relu2"
  type: "ReLU"
  bottom: "layer_64_2_conv1"
  top: "layer_64_2_conv1"
}
layer {
  name: "layer_64_2_conv2"
  type: "Convolution"
  bottom: "layer_64_2_conv1"
  top: "layer_64_2_conv2"
  param {
    name: "weight-64_2_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_2_bn3"
  type: "BatchNorm"
  bottom: "layer_64_2_conv2"
  top: "layer_64_2_conv2"
  param {
    name: "para1-64_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_2_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_2_scale3"
  type: "Scale"
  bottom: "layer_64_2_conv2"
  top: "layer_64_2_conv2"
  param {
    name: "lamda-64_2_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_2_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_2_relu3"
  type: "ReLU"
  bottom: "layer_64_2_conv2"
  top: "layer_64_2_conv2"
}
layer {
  name: "layer_64_2_conv3"
  type: "Convolution"
  bottom: "layer_64_2_conv2"
  top: "layer_64_2_conv3"
  param {
    name: "weight-64_2_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_2_sum"
  type: "Eltwise"
  bottom: "layer_64_2_conv3"
  bottom: "layer_64_1_sum"
  top: "layer_64_2_sum"
}
layer {
  name: "layer_64_3_bn1"
  type: "BatchNorm"
  bottom: "layer_64_2_sum"
  top: "layer_64_3_bn1"
  param {
    name: "para1-64_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_3_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_3_scale1"
  type: "Scale"
  bottom: "layer_64_3_bn1"
  top: "layer_64_3_bn1"
  param {
    name: "lamda-64_3_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_3_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_3_relu1"
  type: "ReLU"
  bottom: "layer_64_3_bn1"
  top: "layer_64_3_bn1"
}
layer {
  name: "layer_64_3_conv1"
  type: "Convolution"
  bottom: "layer_64_3_bn1"
  top: "layer_64_3_conv1"
  param {
    name: "weight-64_3_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_3_bn2"
  type: "BatchNorm"
  bottom: "layer_64_3_conv1"
  top: "layer_64_3_conv1"
  param {
    name: "para1-64_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_3_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_3_scale2"
  type: "Scale"
  bottom: "layer_64_3_conv1"
  top: "layer_64_3_conv1"
  param {
    name: "lamda-64_3_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_3_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_3_relu2"
  type: "ReLU"
  bottom: "layer_64_3_conv1"
  top: "layer_64_3_conv1"
}
layer {
  name: "layer_64_3_conv2"
  type: "Convolution"
  bottom: "layer_64_3_conv1"
  top: "layer_64_3_conv2"
  param {
    name: "weight-64_3_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_3_bn3"
  type: "BatchNorm"
  bottom: "layer_64_3_conv2"
  top: "layer_64_3_conv2"
  param {
    name: "para1-64_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_3_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_3_scale3"
  type: "Scale"
  bottom: "layer_64_3_conv2"
  top: "layer_64_3_conv2"
  param {
    name: "lamda-64_3_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_3_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_3_relu3"
  type: "ReLU"
  bottom: "layer_64_3_conv2"
  top: "layer_64_3_conv2"
}
layer {
  name: "layer_64_3_conv3"
  type: "Convolution"
  bottom: "layer_64_3_conv2"
  top: "layer_64_3_conv3"
  param {
    name: "weight-64_3_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_3_sum"
  type: "Eltwise"
  bottom: "layer_64_3_conv3"
  bottom: "layer_64_2_sum"
  top: "layer_64_3_sum"
}
layer {
  name: "layer_128_1_bn1"
  type: "BatchNorm"
  bottom: "layer_64_3_sum"
  top: "layer_128_1_bn1"
  param {
    name: "para1-128_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_1_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_1_scale1"
  type: "Scale"
  bottom: "layer_128_1_bn1"
  top: "layer_128_1_bn1"
  param {
    name: "lamda-128_1_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_1_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_1_relu1"
  type: "ReLU"
  bottom: "layer_128_1_bn1"
  top: "layer_128_1_bn1"
}
layer {
  name: "layer_128_1_conv1"
  type: "Convolution"
  bottom: "layer_128_1_bn1"
  top: "layer_128_1_conv1"
  param {
    name: "weight-128_1_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_bn2"
  type: "BatchNorm"
  bottom: "layer_128_1_conv1"
  top: "layer_128_1_conv1"
  param {
    name: "para1-128_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_1_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_1_scale2"
  type: "Scale"
  bottom: "layer_128_1_conv1"
  top: "layer_128_1_conv1"
  param {
    name: "lamda-128_1_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_1_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_1_relu2"
  type: "ReLU"
  bottom: "layer_128_1_conv1"
  top: "layer_128_1_conv1"
}
layer {
  name: "layer_128_1_conv2"
  type: "Convolution"
  bottom: "layer_128_1_conv1"
  top: "layer_128_1_conv2"
  param {
    name: "weight-128_1_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_bn3"
  type: "BatchNorm"
  bottom: "layer_128_1_conv2"
  top: "layer_128_1_conv2"
  param {
    name: "para1-128_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_1_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_1_scale3"
  type: "Scale"
  bottom: "layer_128_1_conv2"
  top: "layer_128_1_conv2"
  param {
    name: "lamda-128_1_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_1_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_1_relu3"
  type: "ReLU"
  bottom: "layer_128_1_conv2"
  top: "layer_128_1_conv2"
}
layer {
  name: "layer_128_1_conv3"
  type: "Convolution"
  bottom: "layer_128_1_conv2"
  top: "layer_128_1_conv3"
  param {
    name: "weight-128_1_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_conv_expand"
  type: "Convolution"
  bottom: "layer_128_1_bn1"
  top: "layer_128_1_conv_expand"
  param {
    name: "weight-128_1_conv_expand"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_sum"
  type: "Eltwise"
  bottom: "layer_128_1_conv3"
  bottom: "layer_128_1_conv_expand"
  top: "layer_128_1_sum"
}
layer {
  name: "layer_128_2_bn1"
  type: "BatchNorm"
  bottom: "layer_128_1_sum"
  top: "layer_128_2_bn1"
  param {
    name: "para1-128_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_2_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_2_scale1"
  type: "Scale"
  bottom: "layer_128_2_bn1"
  top: "layer_128_2_bn1"
  param {
    name: "lamda-128_2_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_2_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_2_relu1"
  type: "ReLU"
  bottom: "layer_128_2_bn1"
  top: "layer_128_2_bn1"
}
layer {
  name: "layer_128_2_conv1"
  type: "Convolution"
  bottom: "layer_128_2_bn1"
  top: "layer_128_2_conv1"
  param {
    name: "weight-128_2_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_2_bn2"
  type: "BatchNorm"
  bottom: "layer_128_2_conv1"
  top: "layer_128_2_conv1"
  param {
    name: "para1-128_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_2_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_2_scale2"
  type: "Scale"
  bottom: "layer_128_2_conv1"
  top: "layer_128_2_conv1"
  param {
    name: "lamda-128_2_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_2_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_2_relu2"
  type: "ReLU"
  bottom: "layer_128_2_conv1"
  top: "layer_128_2_conv1"
}
layer {
  name: "layer_128_2_conv2"
  type: "Convolution"
  bottom: "layer_128_2_conv1"
  top: "layer_128_2_conv2"
  param {
    name: "weight-128_2_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_2_bn3"
  type: "BatchNorm"
  bottom: "layer_128_2_conv2"
  top: "layer_128_2_conv2"
  param {
    name: "para1-128_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_2_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_2_scale3"
  type: "Scale"
  bottom: "layer_128_2_conv2"
  top: "layer_128_2_conv2"
  param {
    name: "lamda-128_2_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_2_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_2_relu3"
  type: "ReLU"
  bottom: "layer_128_2_conv2"
  top: "layer_128_2_conv2"
}
layer {
  name: "layer_128_2_conv3"
  type: "Convolution"
  bottom: "layer_128_2_conv2"
  top: "layer_128_2_conv3"
  param {
    name: "weight-128_2_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_2_sum"
  type: "Eltwise"
  bottom: "layer_128_2_conv3"
  bottom: "layer_128_1_sum"
  top: "layer_128_2_sum"
}
layer {
  name: "layer_128_3_bn1"
  type: "BatchNorm"
  bottom: "layer_128_2_sum"
  top: "layer_128_3_bn1"
  param {
    name: "para1-128_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_3_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_3_scale1"
  type: "Scale"
  bottom: "layer_128_3_bn1"
  top: "layer_128_3_bn1"
  param {
    name: "lamda-128_3_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_3_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_3_relu1"
  type: "ReLU"
  bottom: "layer_128_3_bn1"
  top: "layer_128_3_bn1"
}
layer {
  name: "layer_128_3_conv1"
  type: "Convolution"
  bottom: "layer_128_3_bn1"
  top: "layer_128_3_conv1"
  param {
    name: "weight-128_3_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_3_bn2"
  type: "BatchNorm"
  bottom: "layer_128_3_conv1"
  top: "layer_128_3_conv1"
  param {
    name: "para1-128_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_3_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_3_scale2"
  type: "Scale"
  bottom: "layer_128_3_conv1"
  top: "layer_128_3_conv1"
  param {
    name: "lamda-128_3_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_3_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_3_relu2"
  type: "ReLU"
  bottom: "layer_128_3_conv1"
  top: "layer_128_3_conv1"
}
layer {
  name: "layer_128_3_conv2"
  type: "Convolution"
  bottom: "layer_128_3_conv1"
  top: "layer_128_3_conv2"
  param {
    name: "weight-128_3_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_3_bn3"
  type: "BatchNorm"
  bottom: "layer_128_3_conv2"
  top: "layer_128_3_conv2"
  param {
    name: "para1-128_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_3_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_3_scale3"
  type: "Scale"
  bottom: "layer_128_3_conv2"
  top: "layer_128_3_conv2"
  param {
    name: "lamda-128_3_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_3_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_3_relu3"
  type: "ReLU"
  bottom: "layer_128_3_conv2"
  top: "layer_128_3_conv2"
}
layer {
  name: "layer_128_3_conv3"
  type: "Convolution"
  bottom: "layer_128_3_conv2"
  top: "layer_128_3_conv3"
  param {
    name: "weight-128_3_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_3_sum"
  type: "Eltwise"
  bottom: "layer_128_3_conv3"
  bottom: "layer_128_2_sum"
  top: "layer_128_3_sum"
}
layer {
  name: "layer_128_4_bn1"
  type: "BatchNorm"
  bottom: "layer_128_3_sum"
  top: "layer_128_4_bn1"
  param {
    name: "para1-128_4_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_4_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_4_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_4_scale1"
  type: "Scale"
  bottom: "layer_128_4_bn1"
  top: "layer_128_4_bn1"
  param {
    name: "lamda-128_4_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_4_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_4_relu1"
  type: "ReLU"
  bottom: "layer_128_4_bn1"
  top: "layer_128_4_bn1"
}
layer {
  name: "layer_128_4_conv1"
  type: "Convolution"
  bottom: "layer_128_4_bn1"
  top: "layer_128_4_conv1"
  param {
    name: "weight-128_4_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_4_bn2"
  type: "BatchNorm"
  bottom: "layer_128_4_conv1"
  top: "layer_128_4_conv1"
  param {
    name: "para1-128_4_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_4_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_4_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_4_scale2"
  type: "Scale"
  bottom: "layer_128_4_conv1"
  top: "layer_128_4_conv1"
  param {
    name: "lamda-128_4_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_4_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_4_relu2"
  type: "ReLU"
  bottom: "layer_128_4_conv1"
  top: "layer_128_4_conv1"
}
layer {
  name: "layer_128_4_conv2"
  type: "Convolution"
  bottom: "layer_128_4_conv1"
  top: "layer_128_4_conv2"
  param {
    name: "weight-128_4_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_4_bn3"
  type: "BatchNorm"
  bottom: "layer_128_4_conv2"
  top: "layer_128_4_conv2"
  param {
    name: "para1-128_4_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_4_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_4_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_4_scale3"
  type: "Scale"
  bottom: "layer_128_4_conv2"
  top: "layer_128_4_conv2"
  param {
    name: "lamda-128_4_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_4_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_4_relu3"
  type: "ReLU"
  bottom: "layer_128_4_conv2"
  top: "layer_128_4_conv2"
}
layer {
  name: "layer_128_4_conv3"
  type: "Convolution"
  bottom: "layer_128_4_conv2"
  top: "layer_128_4_conv3"
  param {
    name: "weight-128_4_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_4_sum"
  type: "Eltwise"
  bottom: "layer_128_4_conv3"
  bottom: "layer_128_3_sum"
  top: "layer_128_4_sum"
}
layer {
  name: "layer_256_1_bn1"
  type: "BatchNorm"
  bottom: "layer_128_4_sum"
  top: "layer_256_1_bn1"
  param {
    name: "para1-256_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_1_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_1_scale1"
  type: "Scale"
  bottom: "layer_256_1_bn1"
  top: "layer_256_1_bn1"
  param {
    name: "lamda-256_1_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_1_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_1_relu1"
  type: "ReLU"
  bottom: "layer_256_1_bn1"
  top: "layer_256_1_bn1"
}
layer {
  name: "layer_256_1_conv1"
  type: "Convolution"
  bottom: "layer_256_1_bn1"
  top: "layer_256_1_conv1"
  param {
    name: "weight-256_1_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_bn2"
  type: "BatchNorm"
  bottom: "layer_256_1_conv1"
  top: "layer_256_1_conv1"
  param {
    name: "para1-256_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_1_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_1_scale2"
  type: "Scale"
  bottom: "layer_256_1_conv1"
  top: "layer_256_1_conv1"
  param {
    name: "lamda-256_1_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_1_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_1_relu2"
  type: "ReLU"
  bottom: "layer_256_1_conv1"
  top: "layer_256_1_conv1"
}
layer {
  name: "layer_256_1_conv2"
  type: "Convolution"
  bottom: "layer_256_1_conv1"
  top: "layer_256_1_conv2"
  param {
    name: "weight-256_1_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_bn3"
  type: "BatchNorm"
  bottom: "layer_256_1_conv2"
  top: "layer_256_1_conv2"
  param {
    name: "para1-256_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_1_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_1_scale3"
  type: "Scale"
  bottom: "layer_256_1_conv2"
  top: "layer_256_1_conv2"
  param {
    name: "lamda-256_1_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_1_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_1_relu3"
  type: "ReLU"
  bottom: "layer_256_1_conv2"
  top: "layer_256_1_conv2"
}
layer {
  name: "layer_256_1_conv3"
  type: "Convolution"
  bottom: "layer_256_1_conv2"
  top: "layer_256_1_conv3"
  param {
    name: "weight-256_1_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_conv_expand"
  type: "Convolution"
  bottom: "layer_256_1_bn1"
  top: "layer_256_1_conv_expand"
  param {
    name: "weight-256_1_conv_expand"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_sum"
  type: "Eltwise"
  bottom: "layer_256_1_conv3"
  bottom: "layer_256_1_conv_expand"
  top: "layer_256_1_sum"
}
layer {
  name: "layer_256_2_bn1"
  type: "BatchNorm"
  bottom: "layer_256_1_sum"
  top: "layer_256_2_bn1"
  param {
    name: "para1-256_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_2_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_2_scale1"
  type: "Scale"
  bottom: "layer_256_2_bn1"
  top: "layer_256_2_bn1"
  param {
    name: "lamda-256_2_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_2_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_2_relu1"
  type: "ReLU"
  bottom: "layer_256_2_bn1"
  top: "layer_256_2_bn1"
}
layer {
  name: "layer_256_2_conv1"
  type: "Convolution"
  bottom: "layer_256_2_bn1"
  top: "layer_256_2_conv1"
  param {
    name: "weight-256_2_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_2_bn2"
  type: "BatchNorm"
  bottom: "layer_256_2_conv1"
  top: "layer_256_2_conv1"
  param {
    name: "para1-256_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_2_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_2_scale2"
  type: "Scale"
  bottom: "layer_256_2_conv1"
  top: "layer_256_2_conv1"
  param {
    name: "lamda-256_2_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_2_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_2_relu2"
  type: "ReLU"
  bottom: "layer_256_2_conv1"
  top: "layer_256_2_conv1"
}
layer {
  name: "layer_256_2_conv2"
  type: "Convolution"
  bottom: "layer_256_2_conv1"
  top: "layer_256_2_conv2"
  param {
    name: "weight-256_2_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_2_bn3"
  type: "BatchNorm"
  bottom: "layer_256_2_conv2"
  top: "layer_256_2_conv2"
  param {
    name: "para1-256_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_2_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_2_scale3"
  type: "Scale"
  bottom: "layer_256_2_conv2"
  top: "layer_256_2_conv2"
  param {
    name: "lamda-256_2_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_2_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_2_relu3"
  type: "ReLU"
  bottom: "layer_256_2_conv2"
  top: "layer_256_2_conv2"
}
layer {
  name: "layer_256_2_conv3"
  type: "Convolution"
  bottom: "layer_256_2_conv2"
  top: "layer_256_2_conv3"
  param {
    name: "weight-256_2_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_2_sum"
  type: "Eltwise"
  bottom: "layer_256_2_conv3"
  bottom: "layer_256_1_sum"
  top: "layer_256_2_sum"
}
layer {
  name: "layer_256_3_bn1"
  type: "BatchNorm"
  bottom: "layer_256_2_sum"
  top: "layer_256_3_bn1"
  param {
    name: "para1-256_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_3_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_3_scale1"
  type: "Scale"
  bottom: "layer_256_3_bn1"
  top: "layer_256_3_bn1"
  param {
    name: "lamda-256_3_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_3_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_3_relu1"
  type: "ReLU"
  bottom: "layer_256_3_bn1"
  top: "layer_256_3_bn1"
}
layer {
  name: "layer_256_3_conv1"
  type: "Convolution"
  bottom: "layer_256_3_bn1"
  top: "layer_256_3_conv1"
  param {
    name: "weight-256_3_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_3_bn2"
  type: "BatchNorm"
  bottom: "layer_256_3_conv1"
  top: "layer_256_3_conv1"
  param {
    name: "para1-256_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_3_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_3_scale2"
  type: "Scale"
  bottom: "layer_256_3_conv1"
  top: "layer_256_3_conv1"
  param {
    name: "lamda-256_3_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_3_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_3_relu2"
  type: "ReLU"
  bottom: "layer_256_3_conv1"
  top: "layer_256_3_conv1"
}
layer {
  name: "layer_256_3_conv2"
  type: "Convolution"
  bottom: "layer_256_3_conv1"
  top: "layer_256_3_conv2"
  param {
    name: "weight-256_3_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_3_bn3"
  type: "BatchNorm"
  bottom: "layer_256_3_conv2"
  top: "layer_256_3_conv2"
  param {
    name: "para1-256_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_3_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_3_scale3"
  type: "Scale"
  bottom: "layer_256_3_conv2"
  top: "layer_256_3_conv2"
  param {
    name: "lamda-256_3_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_3_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_3_relu3"
  type: "ReLU"
  bottom: "layer_256_3_conv2"
  top: "layer_256_3_conv2"
}
layer {
  name: "layer_256_3_conv3"
  type: "Convolution"
  bottom: "layer_256_3_conv2"
  top: "layer_256_3_conv3"
  param {
    name: "weight-256_3_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_3_sum"
  type: "Eltwise"
  bottom: "layer_256_3_conv3"
  bottom: "layer_256_2_sum"
  top: "layer_256_3_sum"
}
layer {
  name: "layer_256_4_bn1"
  type: "BatchNorm"
  bottom: "layer_256_3_sum"
  top: "layer_256_4_bn1"
  param {
    name: "para1-256_4_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_4_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_4_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_4_scale1"
  type: "Scale"
  bottom: "layer_256_4_bn1"
  top: "layer_256_4_bn1"
  param {
    name: "lamda-256_4_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_4_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_4_relu1"
  type: "ReLU"
  bottom: "layer_256_4_bn1"
  top: "layer_256_4_bn1"
}
layer {
  name: "layer_256_4_conv1"
  type: "Convolution"
  bottom: "layer_256_4_bn1"
  top: "layer_256_4_conv1"
  param {
    name: "weight-256_4_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_4_bn2"
  type: "BatchNorm"
  bottom: "layer_256_4_conv1"
  top: "layer_256_4_conv1"
  param {
    name: "para1-256_4_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_4_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_4_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_4_scale2"
  type: "Scale"
  bottom: "layer_256_4_conv1"
  top: "layer_256_4_conv1"
  param {
    name: "lamda-256_4_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_4_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_4_relu2"
  type: "ReLU"
  bottom: "layer_256_4_conv1"
  top: "layer_256_4_conv1"
}
layer {
  name: "layer_256_4_conv2"
  type: "Convolution"
  bottom: "layer_256_4_conv1"
  top: "layer_256_4_conv2"
  param {
    name: "weight-256_4_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_4_bn3"
  type: "BatchNorm"
  bottom: "layer_256_4_conv2"
  top: "layer_256_4_conv2"
  param {
    name: "para1-256_4_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_4_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_4_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_4_scale3"
  type: "Scale"
  bottom: "layer_256_4_conv2"
  top: "layer_256_4_conv2"
  param {
    name: "lamda-256_4_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_4_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_4_relu3"
  type: "ReLU"
  bottom: "layer_256_4_conv2"
  top: "layer_256_4_conv2"
}
layer {
  name: "layer_256_4_conv3"
  type: "Convolution"
  bottom: "layer_256_4_conv2"
  top: "layer_256_4_conv3"
  param {
    name: "weight-256_4_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_4_sum"
  type: "Eltwise"
  bottom: "layer_256_4_conv3"
  bottom: "layer_256_3_sum"
  top: "layer_256_4_sum"
}
layer {
  name: "layer_256_5_bn1"
  type: "BatchNorm"
  bottom: "layer_256_4_sum"
  top: "layer_256_5_bn1"
  param {
    name: "para1-256_5_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_5_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_5_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_5_scale1"
  type: "Scale"
  bottom: "layer_256_5_bn1"
  top: "layer_256_5_bn1"
  param {
    name: "lamda-256_5_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_5_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_5_relu1"
  type: "ReLU"
  bottom: "layer_256_5_bn1"
  top: "layer_256_5_bn1"
}
layer {
  name: "layer_256_5_conv1"
  type: "Convolution"
  bottom: "layer_256_5_bn1"
  top: "layer_256_5_conv1"
  param {
    name: "weight-256_5_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_5_bn2"
  type: "BatchNorm"
  bottom: "layer_256_5_conv1"
  top: "layer_256_5_conv1"
  param {
    name: "para1-256_5_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_5_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_5_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_5_scale2"
  type: "Scale"
  bottom: "layer_256_5_conv1"
  top: "layer_256_5_conv1"
  param {
    name: "lamda-256_5_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_5_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_5_relu2"
  type: "ReLU"
  bottom: "layer_256_5_conv1"
  top: "layer_256_5_conv1"
}
layer {
  name: "layer_256_5_conv2"
  type: "Convolution"
  bottom: "layer_256_5_conv1"
  top: "layer_256_5_conv2"
  param {
    name: "weight-256_5_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_5_bn3"
  type: "BatchNorm"
  bottom: "layer_256_5_conv2"
  top: "layer_256_5_conv2"
  param {
    name: "para1-256_5_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_5_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_5_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_5_scale3"
  type: "Scale"
  bottom: "layer_256_5_conv2"
  top: "layer_256_5_conv2"
  param {
    name: "lamda-256_5_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_5_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_5_relu3"
  type: "ReLU"
  bottom: "layer_256_5_conv2"
  top: "layer_256_5_conv2"
}
layer {
  name: "layer_256_5_conv3"
  type: "Convolution"
  bottom: "layer_256_5_conv2"
  top: "layer_256_5_conv3"
  param {
    name: "weight-256_5_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_5_sum"
  type: "Eltwise"
  bottom: "layer_256_5_conv3"
  bottom: "layer_256_4_sum"
  top: "layer_256_5_sum"
}
layer {
  name: "layer_256_6_bn1"
  type: "BatchNorm"
  bottom: "layer_256_5_sum"
  top: "layer_256_6_bn1"
  param {
    name: "para1-256_6_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_6_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_6_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_6_scale1"
  type: "Scale"
  bottom: "layer_256_6_bn1"
  top: "layer_256_6_bn1"
  param {
    name: "lamda-256_6_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_6_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_6_relu1"
  type: "ReLU"
  bottom: "layer_256_6_bn1"
  top: "layer_256_6_bn1"
}
layer {
  name: "layer_256_6_conv1"
  type: "Convolution"
  bottom: "layer_256_6_bn1"
  top: "layer_256_6_conv1"
  param {
    name: "weight-256_6_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_6_bn2"
  type: "BatchNorm"
  bottom: "layer_256_6_conv1"
  top: "layer_256_6_conv1"
  param {
    name: "para1-256_6_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_6_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_6_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_6_scale2"
  type: "Scale"
  bottom: "layer_256_6_conv1"
  top: "layer_256_6_conv1"
  param {
    name: "lamda-256_6_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_6_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_6_relu2"
  type: "ReLU"
  bottom: "layer_256_6_conv1"
  top: "layer_256_6_conv1"
}
layer {
  name: "layer_256_6_conv2"
  type: "Convolution"
  bottom: "layer_256_6_conv1"
  top: "layer_256_6_conv2"
  param {
    name: "weight-256_6_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_6_bn3"
  type: "BatchNorm"
  bottom: "layer_256_6_conv2"
  top: "layer_256_6_conv2"
  param {
    name: "para1-256_6_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_6_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_6_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_6_scale3"
  type: "Scale"
  bottom: "layer_256_6_conv2"
  top: "layer_256_6_conv2"
  param {
    name: "lamda-256_6_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_6_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_6_relu3"
  type: "ReLU"
  bottom: "layer_256_6_conv2"
  top: "layer_256_6_conv2"
}
layer {
  name: "layer_256_6_conv3"
  type: "Convolution"
  bottom: "layer_256_6_conv2"
  top: "layer_256_6_conv3"
  param {
    name: "weight-256_6_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_6_sum"
  type: "Eltwise"
  bottom: "layer_256_6_conv3"
  bottom: "layer_256_5_sum"
  top: "layer_256_6_sum"
}
layer {
  name: "layer_512_1_bn1"
  type: "BatchNorm"
  bottom: "layer_256_6_sum"
  top: "layer_512_1_bn1"
  param {
    name: "para1-512_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_1_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_1_scale1"
  type: "Scale"
  bottom: "layer_512_1_bn1"
  top: "layer_512_1_bn1"
  param {
    name: "lamda-512_1_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_1_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_1_relu1"
  type: "ReLU"
  bottom: "layer_512_1_bn1"
  top: "layer_512_1_bn1"
}
layer {
  name: "layer_512_1_conv1"
  type: "Convolution"
  bottom: "layer_512_1_bn1"
  top: "layer_512_1_conv1"
  param {
    name: "weight-512_1_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_bn2"
  type: "BatchNorm"
  bottom: "layer_512_1_conv1"
  top: "layer_512_1_conv1"
  param {
    name: "para1-512_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_1_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_1_scale2"
  type: "Scale"
  bottom: "layer_512_1_conv1"
  top: "layer_512_1_conv1"
  param {
    name: "lamda-512_1_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_1_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_1_relu2"
  type: "ReLU"
  bottom: "layer_512_1_conv1"
  top: "layer_512_1_conv1"
}
layer {
  name: "layer_512_1_conv2"
  type: "Convolution"
  bottom: "layer_512_1_conv1"
  top: "layer_512_1_conv2"
  param {
    name: "weight-512_1_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_bn3"
  type: "BatchNorm"
  bottom: "layer_512_1_conv2"
  top: "layer_512_1_conv2"
  param {
    name: "para1-512_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_1_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_1_scale3"
  type: "Scale"
  bottom: "layer_512_1_conv2"
  top: "layer_512_1_conv2"
  param {
    name: "lamda-512_1_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_1_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_1_relu3"
  type: "ReLU"
  bottom: "layer_512_1_conv2"
  top: "layer_512_1_conv2"
}
layer {
  name: "layer_512_1_conv3"
  type: "Convolution"
  bottom: "layer_512_1_conv2"
  top: "layer_512_1_conv3"
  param {
    name: "weight-512_1_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 2048
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_conv_expand"
  type: "Convolution"
  bottom: "layer_512_1_bn1"
  top: "layer_512_1_conv_expand"
  param {
    name: "weight-512_1_conv_expand"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 2048
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_sum"
  type: "Eltwise"
  bottom: "layer_512_1_conv3"
  bottom: "layer_512_1_conv_expand"
  top: "layer_512_1_sum"
}
layer {
  name: "layer_512_2_bn1"
  type: "BatchNorm"
  bottom: "layer_512_1_sum"
  top: "layer_512_2_bn1"
  param {
    name: "para1-512_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_2_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_2_scale1"
  type: "Scale"
  bottom: "layer_512_2_bn1"
  top: "layer_512_2_bn1"
  param {
    name: "lamda-512_2_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_2_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_2_relu1"
  type: "ReLU"
  bottom: "layer_512_2_bn1"
  top: "layer_512_2_bn1"
}
layer {
  name: "layer_512_2_conv1"
  type: "Convolution"
  bottom: "layer_512_2_bn1"
  top: "layer_512_2_conv1"
  param {
    name: "weight-512_2_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_2_bn2"
  type: "BatchNorm"
  bottom: "layer_512_2_conv1"
  top: "layer_512_2_conv1"
  param {
    name: "para1-512_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_2_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_2_scale2"
  type: "Scale"
  bottom: "layer_512_2_conv1"
  top: "layer_512_2_conv1"
  param {
    name: "lamda-512_2_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_2_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_2_relu2"
  type: "ReLU"
  bottom: "layer_512_2_conv1"
  top: "layer_512_2_conv1"
}
layer {
  name: "layer_512_2_conv2"
  type: "Convolution"
  bottom: "layer_512_2_conv1"
  top: "layer_512_2_conv2"
  param {
    name: "weight-512_2_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_2_bn3"
  type: "BatchNorm"
  bottom: "layer_512_2_conv2"
  top: "layer_512_2_conv2"
  param {
    name: "para1-512_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_2_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_2_scale3"
  type: "Scale"
  bottom: "layer_512_2_conv2"
  top: "layer_512_2_conv2"
  param {
    name: "lamda-512_2_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_2_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_2_relu3"
  type: "ReLU"
  bottom: "layer_512_2_conv2"
  top: "layer_512_2_conv2"
}
layer {
  name: "layer_512_2_conv3"
  type: "Convolution"
  bottom: "layer_512_2_conv2"
  top: "layer_512_2_conv3"
  param {
    name: "weight-512_2_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 2048
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_2_sum"
  type: "Eltwise"
  bottom: "layer_512_2_conv3"
  bottom: "layer_512_1_sum"
  top: "layer_512_2_sum"
}
layer {
  name: "layer_512_3_bn1"
  type: "BatchNorm"
  bottom: "layer_512_2_sum"
  top: "layer_512_3_bn1"
  param {
    name: "para1-512_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_3_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_3_scale1"
  type: "Scale"
  bottom: "layer_512_3_bn1"
  top: "layer_512_3_bn1"
  param {
    name: "lamda-512_3_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_3_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_3_relu1"
  type: "ReLU"
  bottom: "layer_512_3_bn1"
  top: "layer_512_3_bn1"
}
layer {
  name: "layer_512_3_conv1"
  type: "Convolution"
  bottom: "layer_512_3_bn1"
  top: "layer_512_3_conv1"
  param {
    name: "weight-512_3_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_3_bn2"
  type: "BatchNorm"
  bottom: "layer_512_3_conv1"
  top: "layer_512_3_conv1"
  param {
    name: "para1-512_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_3_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_3_scale2"
  type: "Scale"
  bottom: "layer_512_3_conv1"
  top: "layer_512_3_conv1"
  param {
    name: "lamda-512_3_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_3_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_3_relu2"
  type: "ReLU"
  bottom: "layer_512_3_conv1"
  top: "layer_512_3_conv1"
}
layer {
  name: "layer_512_3_conv2"
  type: "Convolution"
  bottom: "layer_512_3_conv1"
  top: "layer_512_3_conv2"
  param {
    name: "weight-512_3_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_3_bn3"
  type: "BatchNorm"
  bottom: "layer_512_3_conv2"
  top: "layer_512_3_conv2"
  param {
    name: "para1-512_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_3_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_3_scale3"
  type: "Scale"
  bottom: "layer_512_3_conv2"
  top: "layer_512_3_conv2"
  param {
    name: "lamda-512_3_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_3_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_3_relu3"
  type: "ReLU"
  bottom: "layer_512_3_conv2"
  top: "layer_512_3_conv2"
}
layer {
  name: "layer_512_3_conv3"
  type: "Convolution"
  bottom: "layer_512_3_conv2"
  top: "layer_512_3_conv3"
  param {
    name: "weight-512_3_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 2048
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_3_sum"
  type: "Eltwise"
  bottom: "layer_512_3_conv3"
  bottom: "layer_512_2_sum"
  top: "layer_512_3_sum"
}
layer {
  name: "last_bn"
  type: "BatchNorm"
  bottom: "layer_512_3_sum"
  top: "layer_512_3_sum"
  param {
    name: "para1-last_bn"
    lr_mult: 0.0
  }
  param {
    name: "para2-last_bn"
    lr_mult: 0.0
  }
  param {
    name: "para3-last_bn"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "last_scale"
  type: "Scale"
  bottom: "layer_512_3_sum"
  top: "layer_512_3_sum"
  param {
    name: "lamda-last_scale"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-last_scale"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "last_relu"
  type: "ReLU"
  bottom: "layer_512_3_sum"
  top: "layer_512_3_sum"
}
#
# conv5 output
#
layer {
  name: "conv6"
  type: "Convolution"
  bottom: "layer_512_3_sum"
  top: "conv6"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
# 
# spatial attention
#
layer {
  name: "conv5_4/bn-pool1"
  type: "Interp"
  bottom: "layer_512_3_sum"
  top: "conv5_4/bn-pool1"
  interp_param {
    height: 6
    width: 6
  }
}

layer {
  name: "spa.att_Conv_1"
  type: "Convolution"
  bottom: "conv5_4/bn-pool1"
  top: "spa.att_Conv_1"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_1-norm"
  type: "Normalize"
  bottom: "spa.att_Conv_1"
  top: "spa.att_Conv_1-norm"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_2"
  type: "Convolution"
  bottom: "conv5_4/bn-pool1"
  top: "spa.att_Conv_2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_2-norm"
  type: "Normalize"
  bottom: "spa.att_Conv_2"
  top: "spa.att_Conv_2-norm"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_3"
  type: "Convolution"
  bottom: "conv5_4/bn-pool1"
  top: "spa.att_Conv_3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_3-norm"
  type: "Normalize"
  bottom: "spa.att_Conv_3"
  top: "spa.att_Conv_3-norm"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_all-norm"
  type: "Concat"
  bottom: "spa.att_Conv_1-norm"
  bottom: "spa.att_Conv_2-norm"
  bottom: "spa.att_Conv_3-norm"
  top: "spa.att_all-norm"
}
layer {
  name: "spa.att_all-norm_mask"
  type: "Convolution"
  bottom: "spa.att_all-norm"
  top: "spa.att_all-norm_mask"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 1
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Reshape_4"
  type: "Reshape"
  bottom: "spa.att_all-norm_mask"
  top: "spa.att_Reshape_4"
  reshape_param {
    shape {
      dim: -1
      dim: 36
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "spa.att_softmax_5"
  type: "Softmax"
  bottom: "spa.att_Reshape_4"
  top: "spa.att_softmax_5"
  softmax_param {
    axis: 1
  }
}
layer {
  name: "spa.att_Reshape_6"
  type: "Reshape"
  bottom: "spa.att_softmax_5"
  top: "spa.att_Reshape_6"
  reshape_param {
    shape {
      dim: -1
      dim: 1
      dim: 6
      dim: 6
    }
  }
}
layer {
  name: "spa.att_Reshape_6-power"
  type: "Power"
  bottom: "spa.att_Reshape_6"
  top: "spa.att_Reshape_6-power"
  power_param {
    power: 1
    scale: 36
    shift: 0
  }
}
layer {
  name: "spa.att_Reshape_6-power_interp"
  type: "Interp"
  bottom: "spa.att_Reshape_6-power"
  top: "spa.att_Reshape_6-power_interp"
  interp_param {
    height: 12
    width: 12
  }
}
layer {
  name: "spa.att_Reshape_6-power_tile"
  type: "Tile"
  bottom: "spa.att_Reshape_6-power_interp"
  top: "spa.att_Reshape_6-power_tile"
  tile_param {
    tiles: 128
  }
}
# 
# spatial end
#
layer {
  name: "conv6_prod"
  type: "Eltwise"
  bottom: "conv6"
  bottom: "spa.att_Reshape_6-power_tile"
  top: "conv6_prod"
  eltwise_param {
    operation: PROD
  }
}
#
#
#
#
# conv4 output
#
layer {
  name: "conv6-c4"
  type: "Convolution"
  bottom: "layer_512_1_bn1"
  top: "conv6-c4"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
# 
# spatial attention
#
layer {
  name: "conv4_4/bn-pool1-c4"
  type: "Interp"
  bottom: "layer_512_1_bn1"
  top: "conv4_4/bn-pool1-c4"
  interp_param {
    height: 12
    width: 12
  }
}

layer {
  name: "spa.att_Conv_1-c4"
  type: "Convolution"
  bottom: "conv4_4/bn-pool1-c4"
  top: "spa.att_Conv_1-c4"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_1-norm-c4"
  type: "Normalize"
  bottom: "spa.att_Conv_1-c4"
  top: "spa.att_Conv_1-norm-c4"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_2-c4"
  type: "Convolution"
  bottom: "conv4_4/bn-pool1-c4"
  top: "spa.att_Conv_2-c4"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_2-norm-c4"
  type: "Normalize"
  bottom: "spa.att_Conv_2-c4"
  top: "spa.att_Conv_2-norm-c4"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_3-c4"
  type: "Convolution"
  bottom: "conv4_4/bn-pool1-c4"
  top: "spa.att_Conv_3-c4"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_3-norm-c4"
  type: "Normalize"
  bottom: "spa.att_Conv_3-c4"
  top: "spa.att_Conv_3-norm-c4"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_all-norm-c4"
  type: "Concat"
  bottom: "spa.att_Conv_1-norm-c4"
  bottom: "spa.att_Conv_2-norm-c4"
  bottom: "spa.att_Conv_3-norm-c4"
  top: "spa.att_all-norm-c4"
}
layer {
  name: "spa.att_all-norm_mask-c4"
  type: "Convolution"
  bottom: "spa.att_all-norm-c4"
  top: "spa.att_all-norm_mask-c4"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 1
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Reshape_4-c4"
  type: "Reshape"
  bottom: "spa.att_all-norm_mask-c4"
  top: "spa.att_Reshape_4-c4"
  reshape_param {
    shape {
      dim: -1
      dim: 144
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "spa.att_softmax_5-c4"
  type: "Softmax"
  bottom: "spa.att_Reshape_4-c4"
  top: "spa.att_softmax_5-c4"
  softmax_param {
    axis: 1
  }
}
layer {
  name: "spa.att_Reshape_6-c4"
  type: "Reshape"
  bottom: "spa.att_softmax_5-c4"
  top: "spa.att_Reshape_6-c4"
  reshape_param {
    shape {
      dim: -1
      dim: 1
      dim: 12
      dim: 12
    }
  }
}
layer {
  name: "spa.att_Reshape_6-power-c4"
  type: "Power"
  bottom: "spa.att_Reshape_6-c4"
  top: "spa.att_Reshape_6-power-c4"
  power_param {
    power: 1
    scale: 144
    shift: 0
  }
}
layer {
  name: "spa.att_Reshape_6-power_interp-c4"
  type: "Interp"
  bottom: "spa.att_Reshape_6-power-c4"
  top: "spa.att_Reshape_6-power_interp-c4"
  interp_param {
    height: 24
    width: 24
  }
}
layer {
  name: "spa.att_Reshape_6-power_tile-c4"
  type: "Tile"
  bottom: "spa.att_Reshape_6-power_interp-c4"
  top: "spa.att_Reshape_6-power_tile-c4"
  tile_param {
    tiles: 128
  }
}
# 
# spatial end
#
layer {
  name: "conv6_prod-c4"
  type: "Eltwise"
  bottom: "conv6-c4"
  bottom: "spa.att_Reshape_6-power_tile-c4"
  top: "conv6_prod-c4"
  eltwise_param {
    operation: PROD
  }
}

#
#
# conv3 output
#
layer {
  name: "conv6-c3"
  type: "Convolution"
  bottom: "layer_256_1_bn1"
  top: "conv6-c3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
# 
# spatial attention
#
layer {
  name: "conv3_4/bn-pool1-c3"
  type: "Interp"
  bottom: "layer_256_1_bn1"
  top: "conv3_4/bn-pool1-c3"
  interp_param {
    height: 24
    width: 24
  }
}

layer {
  name: "spa.att_Conv_1-c3"
  type: "Convolution"
  bottom: "conv3_4/bn-pool1-c3"
  top: "spa.att_Conv_1-c3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_1-norm-c3"
  type: "Normalize"
  bottom: "spa.att_Conv_1-c3"
  top: "spa.att_Conv_1-norm-c3"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_2-c3"
  type: "Convolution"
  bottom: "conv3_4/bn-pool1-c3"
  top: "spa.att_Conv_2-c3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_2-norm-c3"
  type: "Normalize"
  bottom: "spa.att_Conv_2-c3"
  top: "spa.att_Conv_2-norm-c3"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_3-c3"
  type: "Convolution"
  bottom: "conv3_4/bn-pool1-c3"
  top: "spa.att_Conv_3-c3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_3-norm-c3"
  type: "Normalize"
  bottom: "spa.att_Conv_3-c3"
  top: "spa.att_Conv_3-norm-c3"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_all-norm-c3"
  type: "Concat"
  bottom: "spa.att_Conv_1-norm-c3"
  bottom: "spa.att_Conv_2-norm-c3"
  bottom: "spa.att_Conv_3-norm-c3"
  top: "spa.att_all-norm-c3"
}
layer {
  name: "spa.att_all-norm_mask-c3"
  type: "Convolution"
  bottom: "spa.att_all-norm-c3"
  top: "spa.att_all-norm_mask-c3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 1
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Reshape_4-c3"
  type: "Reshape"
  bottom: "spa.att_all-norm_mask-c3"
  top: "spa.att_Reshape_4-c3"
  reshape_param {
    shape {
      dim: -1
      dim: 576
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "spa.att_softmax_5-c3"
  type: "Softmax"
  bottom: "spa.att_Reshape_4-c3"
  top: "spa.att_softmax_5-c3"
  softmax_param {
    axis: 1
  }
}
layer {
  name: "spa.att_Reshape_6-c3"
  type: "Reshape"
  bottom: "spa.att_softmax_5-c3"
  top: "spa.att_Reshape_6-c3"
  reshape_param {
    shape {
      dim: -1
      dim: 1
      dim: 24
      dim: 24
    }
  }
}
layer {
  name: "spa.att_Reshape_6-power-c3"
  type: "Power"
  bottom: "spa.att_Reshape_6-c3"
  top: "spa.att_Reshape_6-power-c3"
  power_param {
    power: 1
    scale: 576
    shift: 0
  }
}
layer {
  name: "spa.att_Reshape_6-power_interp-c3"
  type: "Interp"
  bottom: "spa.att_Reshape_6-power-c3"
  top: "spa.att_Reshape_6-power_interp-c3"
  interp_param {
    height: 48
    width: 48
  }
}
layer {
  name: "spa.att_Reshape_6-power_tile-c3"
  type: "Tile"
  bottom: "spa.att_Reshape_6-power_interp-c3"
  top: "spa.att_Reshape_6-power_tile-c3"
  tile_param {
    tiles: 128
  }
}
# 
# spatial end
#
layer {
  name: "conv6_prod-c3"
  type: "Eltwise"
  bottom: "conv6-c3"
  bottom: "spa.att_Reshape_6-power_tile-c3"
  top: "conv6_prod-c3"
  eltwise_param {
    operation: PROD
  }
}
#
#
#
# three scale concat
#
layer {
  name: "conv6_prod_interp"
  type: "Interp"
  bottom: "conv6_prod"
  top: "conv6_prod_interp"
  interp_param {
    height: 48
    width: 48
  }
}
layer {
  name: "conv6_prod-c4_interp"
  type: "Interp"
  bottom: "conv6_prod-c4"
  top: "conv6_prod-c4_interp"
  interp_param {
    height: 48
    width: 48
  }
}

layer {
  name: "conv6_prod-concat"
  type: "Eltwise"
  bottom: "conv6_prod-c3"
  bottom: "conv6_prod-c4_interp"
  bottom: "conv6_prod_interp"
  top: "conv6_prod-concat"

}
layer {
  name: "conv7"
  type: "Convolution"
  bottom: "conv6_prod-concat"
  top: "conv7"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "conv7-2"
  type: "Convolution"
  bottom: "conv7"
  top: "conv7-2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  convolution_param {
    num_output: 2
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "conv7_interp"
  type: "Interp"
  bottom: "conv7-2"
  top: "conv7_interp"
  interp_param {
    height: 384
    width: 384
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "conv7_interp"
  bottom: "label"
  top: "loss"
  loss_param {
#    ignore_label: 255
    normalize: false
  }
}
#
# net2
#
layer {
  name: "data-net2"
  type: "ImageData"
  top: "data-net2"
  top: "label_img1-net2"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_value: 0.0
    mean_value: 0.0
    mean_value: 0.0
  }
  image_data_param {
    source: "/media/wtt/Data/datasets/train_duts.txt"
    batch_size: 1
    new_height: 384
    new_width: 384
  }
}
layer {
  name: "label-net2"
  type: "ImageData"
  top: "label-net2"
  top: "label_gt-net2"
  include {
    phase: TRAIN
  }
  image_data_param {
    source: "/media/wtt/Data/datasets/train_duts_gt01.txt"
    is_color: false
    batch_size: 1
    new_height: 384
    new_width: 384
  }
}
layer {
  name: "label_img1-net2"
  type: "Silence"
  bottom: "label_img1-net2"
}
layer {
  name: "label_gt-net2"
  type: "Silence"
  bottom: "label_gt-net2"
}
layer {
  name: "data_bn-net2"
  type: "BatchNorm"
  bottom: "data-net2"
  top: "data_bn-net2"
  param {
    name: "para1-data"
    lr_mult: 0.0
  }
  param {
    name: "para2-data"
    lr_mult: 0.0
  }
  param {
    name: "para3-data"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "data_scale-net2"
  type: "Scale"
  bottom: "data_bn-net2"
  top: "data_bn-net2"
  param {
    name: "lamda-data"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-data"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}

layer {
  name: "conv1-net2"
  type: "Convolution"
  bottom: "data_bn-net2"
  top: "conv1-net2"
  param {
    name: "weight-c1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "bias-c1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    pad: 3
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "msra"
      variance_norm: FAN_OUT
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "conv1_bn-net2"
  type: "BatchNorm"
  bottom: "conv1-net2"
  top: "conv1-net2"
  param {
    name: "para1-c1"
    lr_mult: 0.0
  }
  param {
    name: "para2-c1"
    lr_mult: 0.0
  }
  param {
    name: "para3-c1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv1_scale-net2"
  type: "Scale"
  bottom: "conv1-net2"
  top: "conv1-net2"
  param {
    name: "lamda-c1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-c1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1_relu-net2"
  type: "ReLU"
  bottom: "conv1-net2"
  top: "conv1-net2"
}
layer {
  name: "layer_128_1_bn1-c"
  type: "Convolution"
  bottom: "layer_128_1_bn1"
  top: "layer_128_1_bn1-c"
  param {
    name: "weight-layer_128_1_bn1-c"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_bn1-interp"
  type: "Interp"
  bottom: "layer_128_1_bn1-c"
  top: "layer_128_1_bn1-c-interp"
  interp_param {
    height: 192
    width: 192
  }
}
layer {
  name: "conv2-add"
  type: "Eltwise"
  bottom: "conv1-net2"
  bottom: "layer_128_1_bn1-c-interp"
  top: "conv2-add"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv1_pool-net2"
  type: "Pooling"
  bottom: "conv2-add"
  top: "conv1_pool-net2"
  pooling_param {
    kernel_size: 3
    stride: 2
    pad: 0
  }
}

layer {
  name: "layer_64_1_conv1-net2"
  type: "Convolution"
  bottom: "conv1_pool-net2"
  top: "layer_64_1_conv1-net2"
  param {
    name: "weight-64_1_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_1_bn2-net2"
  type: "BatchNorm"
  bottom: "layer_64_1_conv1-net2"
  top: "layer_64_1_conv1-net2"
  param {
    name: "para1-64_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_1_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_1_scale2-net2"
  type: "Scale"
  bottom: "layer_64_1_conv1-net2"
  top: "layer_64_1_conv1-net2"
  param {
    name: "lamda-64_1_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_1_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_1_relu2-net2"
  type: "ReLU"
  bottom: "layer_64_1_conv1-net2"
  top: "layer_64_1_conv1-net2"
}
layer {
  name: "layer_64_1_conv2-net2"
  type: "Convolution"
  bottom: "layer_64_1_conv1-net2"
  top: "layer_64_1_conv2-net2"
  param {
    name: "weight-64_1_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_1_bn3-net2"
  type: "BatchNorm"
  bottom: "layer_64_1_conv2-net2"
  top: "layer_64_1_conv2-net2"
  param {
    name: "para1-64_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_1_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_1_scale3-net2"
  type: "Scale"
  bottom: "layer_64_1_conv2-net2"
  top: "layer_64_1_conv2-net2"
  param {
    name: "lamda-64_1_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_1_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_1_relu3-net2"
  type: "ReLU"
  bottom: "layer_64_1_conv2-net2"
  top: "layer_64_1_conv2-net2"
}
layer {
  name: "layer_64_1_conv3-net2"
  type: "Convolution"
  bottom: "layer_64_1_conv2-net2"
  top: "layer_64_1_conv3-net2"
  param {
    name: "weight-64_1_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_1_conv_expand-net2"
  type: "Convolution"
  bottom: "layer_64_1_conv1-net2"
  top: "layer_64_1_conv_expand-net2"
  param {
    name: "weight-64_1_conv_expand"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_1_sum-net2"
  type: "Eltwise"
  bottom: "layer_64_1_conv3-net2"
  bottom: "layer_64_1_conv_expand-net2"
  top: "layer_64_1_sum-net2"
}
layer {
  name: "layer_64_2_bn1-net2"
  type: "BatchNorm"
  bottom: "layer_64_1_sum-net2"
  top: "layer_64_2_bn1-net2"
  param {
    name: "para1-64_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_2_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_2_scale1-net2"
  type: "Scale"
  bottom: "layer_64_2_bn1-net2"
  top: "layer_64_2_bn1-net2"
  param {
    name: "lamda-64_2_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_2_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_2_relu1-net2"
  type: "ReLU"
  bottom: "layer_64_2_bn1-net2"
  top: "layer_64_2_bn1-net2"
}
layer {
  name: "layer_64_2_conv1-net2"
  type: "Convolution"
  bottom: "layer_64_2_bn1-net2"
  top: "layer_64_2_conv1-net2"
  param {
    name: "weight-64_2_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_2_bn2-net2"
  type: "BatchNorm"
  bottom: "layer_64_2_conv1-net2"
  top: "layer_64_2_conv1-net2"
  param {
    name: "para1-64_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_2_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_2_scale2-net2"
  type: "Scale"
  bottom: "layer_64_2_conv1-net2"
  top: "layer_64_2_conv1-net2"
  param {
    name: "lamda-64_2_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_2_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_2_relu2-net2"
  type: "ReLU"
  bottom: "layer_64_2_conv1-net2"
  top: "layer_64_2_conv1-net2"
}
layer {
  name: "layer_64_2_conv2-net2"
  type: "Convolution"
  bottom: "layer_64_2_conv1-net2"
  top: "layer_64_2_conv2-net2"
  param {
    name: "weight-64_2_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_2_bn3-net2"
  type: "BatchNorm"
  bottom: "layer_64_2_conv2-net2"
  top: "layer_64_2_conv2-net2"
  param {
    name: "para1-64_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_2_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_2_scale3-net2"
  type: "Scale"
  bottom: "layer_64_2_conv2-net2"
  top: "layer_64_2_conv2-net2"
  param {
    name: "lamda-64_2_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_2_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_2_relu3-net2"
  type: "ReLU"
  bottom: "layer_64_2_conv2-net2"
  top: "layer_64_2_conv2-net2"
}
layer {
  name: "layer_64_2_conv3-net2"
  type: "Convolution"
  bottom: "layer_64_2_conv2-net2"
  top: "layer_64_2_conv3-net2"
  param {
    name: "weight-64_2_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_2_sum-net2"
  type: "Eltwise"
  bottom: "layer_64_2_conv3-net2"
  bottom: "layer_64_1_sum-net2"
  top: "layer_64_2_sum-net2"
}
layer {
  name: "layer_64_3_bn1-net2"
  type: "BatchNorm"
  bottom: "layer_64_2_sum-net2"
  top: "layer_64_3_bn1-net2"
  param {
    name: "para1-64_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_3_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_3_scale1-net2"
  type: "Scale"
  bottom: "layer_64_3_bn1-net2"
  top: "layer_64_3_bn1-net2"
  param {
    name: "lamda-64_3_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_3_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_3_relu1-net2"
  type: "ReLU"
  bottom: "layer_64_3_bn1-net2"
  top: "layer_64_3_bn1-net2"
}
layer {
  name: "layer_64_3_conv1-net2"
  type: "Convolution"
  bottom: "layer_64_3_bn1-net2"
  top: "layer_64_3_conv1-net2"
  param {
    name: "weight-64_3_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_3_bn2-net2"
  type: "BatchNorm"
  bottom: "layer_64_3_conv1-net2"
  top: "layer_64_3_conv1-net2"
  param {
    name: "para1-64_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_3_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_3_scale2-net2"
  type: "Scale"
  bottom: "layer_64_3_conv1-net2"
  top: "layer_64_3_conv1-net2"
  param {
    name: "lamda-64_3_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_3_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_3_relu2-net2"
  type: "ReLU"
  bottom: "layer_64_3_conv1-net2"
  top: "layer_64_3_conv1-net2"
}
layer {
  name: "layer_64_3_conv2-net2"
  type: "Convolution"
  bottom: "layer_64_3_conv1-net2"
  top: "layer_64_3_conv2-net2"
  param {
    name: "weight-64_3_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_3_bn3-net2"
  type: "BatchNorm"
  bottom: "layer_64_3_conv2-net2"
  top: "layer_64_3_conv2-net2"
  param {
    name: "para1-64_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_3_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_3_scale3-net2"
  type: "Scale"
  bottom: "layer_64_3_conv2-net2"
  top: "layer_64_3_conv2-net2"
  param {
    name: "lamda-64_3_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_3_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_3_relu3-net2"
  type: "ReLU"
  bottom: "layer_64_3_conv2-net2"
  top: "layer_64_3_conv2-net2"
}
layer {
  name: "layer_64_3_conv3-net2"
  type: "Convolution"
  bottom: "layer_64_3_conv2-net2"
  top: "layer_64_3_conv3-net2"
  param {
    name: "weight-64_3_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_3_sum-net2"
  type: "Eltwise"
  bottom: "layer_64_3_conv3-net2"
  bottom: "layer_64_2_sum-net2"
  top: "layer_64_3_sum-net2"
}
layer {
  name: "layer_128_1_bn1-net2"
  type: "BatchNorm"
  bottom: "layer_64_3_sum-net2"
  top: "layer_128_1_bn1-net2"
  param {
    name: "para1-128_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_1_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_1_scale1-net2"
  type: "Scale"
  bottom: "layer_128_1_bn1-net2"
  top: "layer_128_1_bn1-net2"
  param {
    name: "lamda-128_1_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_1_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_1_relu1-net2"
  type: "ReLU"
  bottom: "layer_128_1_bn1-net2"
  top: "layer_128_1_bn1-net2"
}
layer {
  name: "layer_256_1_bn1-c"
  type: "Convolution"
  bottom: "layer_256_1_bn1"
  top: "layer_256_1_bn1-c"
  param {
    name: "weight-layer_256_1_bn1-c"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_bn1-c-interp"
  type: "Interp"
  bottom: "layer_256_1_bn1-c"
  top: "layer_256_1_bn1-c-interp"
  interp_param {
    height: 96
    width: 96
  }
}
layer {
  name: "conv3-add"
  type: "Eltwise"
  bottom: "layer_128_1_bn1-net2"
  bottom: "layer_256_1_bn1-c-interp"
  top: "conv3-add"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "layer_128_1_conv1-net2"
  type: "Convolution"
  bottom: "conv3-add"
  top: "layer_128_1_conv1-net2"
  param {
    name: "weight-128_1_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_bn2-net2"
  type: "BatchNorm"
  bottom: "layer_128_1_conv1-net2"
  top: "layer_128_1_conv1-net2"
  param {
    name: "para1-128_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_1_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_1_scale2-net2"
  type: "Scale"
  bottom: "layer_128_1_conv1-net2"
  top: "layer_128_1_conv1-net2"
  param {
    name: "lamda-128_1_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_1_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_1_relu2-net2"
  type: "ReLU"
  bottom: "layer_128_1_conv1-net2"
  top: "layer_128_1_conv1-net2"
}
layer {
  name: "layer_128_1_conv2-net2"
  type: "Convolution"
  bottom: "layer_128_1_conv1-net2"
  top: "layer_128_1_conv2-net2"
  param {
    name: "weight-128_1_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_bn3-net2"
  type: "BatchNorm"
  bottom: "layer_128_1_conv2-net2"
  top: "layer_128_1_conv2-net2"
  param {
    name: "para1-128_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_1_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_1_scale3-net2"
  type: "Scale"
  bottom: "layer_128_1_conv2-net2"
  top: "layer_128_1_conv2-net2"
  param {
    name: "lamda-128_1_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_1_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_1_relu3-net2"
  type: "ReLU"
  bottom: "layer_128_1_conv2-net2"
  top: "layer_128_1_conv2-net2"
}
layer {
  name: "layer_128_1_conv3-net2"
  type: "Convolution"
  bottom: "layer_128_1_conv2-net2"
  top: "layer_128_1_conv3-net2"
  param {
    name: "weight-128_1_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_conv_expand-net2"
  type: "Convolution"
  bottom: "conv3-add"
  top: "layer_128_1_conv_expand-net2"
  param {
    name: "weight-128_1_conv_expand"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_sum-net2"
  type: "Eltwise"
  bottom: "layer_128_1_conv3-net2"
  bottom: "layer_128_1_conv_expand-net2"
  top: "layer_128_1_sum-net2"
}

layer {
  name: "layer_128_2_bn1-net2"
  type: "BatchNorm"
  bottom: "layer_128_1_sum-net2"
  top: "layer_128_2_bn1-net2"
  param {
    name: "para1-128_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_2_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_2_scale1-net2"
  type: "Scale"
  bottom: "layer_128_2_bn1-net2"
  top: "layer_128_2_bn1-net2"
  param {
    name: "lamda-128_2_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_2_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_2_relu1-net2"
  type: "ReLU"
  bottom: "layer_128_2_bn1-net2"
  top: "layer_128_2_bn1-net2"
}
layer {
  name: "layer_128_2_conv1-net2"
  type: "Convolution"
  bottom: "layer_128_2_bn1-net2"
  top: "layer_128_2_conv1-net2"
  param {
    name: "weight-128_2_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_2_bn2-net2"
  type: "BatchNorm"
  bottom: "layer_128_2_conv1-net2"
  top: "layer_128_2_conv1-net2"
  param {
    name: "para1-128_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_2_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_2_scale2-net2"
  type: "Scale"
  bottom: "layer_128_2_conv1-net2"
  top: "layer_128_2_conv1-net2"
  param {
    name: "lamda-128_2_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_2_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_2_relu2-net2"
  type: "ReLU"
  bottom: "layer_128_2_conv1-net2"
  top: "layer_128_2_conv1-net2"
}
layer {
  name: "layer_128_2_conv2-net2"
  type: "Convolution"
  bottom: "layer_128_2_conv1-net2"
  top: "layer_128_2_conv2-net2"
  param {
    name: "weight-128_2_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_2_bn3-net2"
  type: "BatchNorm"
  bottom: "layer_128_2_conv2-net2"
  top: "layer_128_2_conv2-net2"
  param {
    name: "para1-128_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_2_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_2_scale3-net2"
  type: "Scale"
  bottom: "layer_128_2_conv2-net2"
  top: "layer_128_2_conv2-net2"
  param {
    name: "lamda-128_2_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_2_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_2_relu3-net2"
  type: "ReLU"
  bottom: "layer_128_2_conv2-net2"
  top: "layer_128_2_conv2-net2"
}
layer {
  name: "layer_128_2_conv3-net2"
  type: "Convolution"
  bottom: "layer_128_2_conv2-net2"
  top: "layer_128_2_conv3-net2"
  param {
    name: "weight-128_2_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_2_sum-net2"
  type: "Eltwise"
  bottom: "layer_128_2_conv3-net2"
  bottom: "layer_128_1_sum-net2"
  top: "layer_128_2_sum-net2"
}
layer {
  name: "layer_128_3_bn1-net2"
  type: "BatchNorm"
  bottom: "layer_128_2_sum-net2"
  top: "layer_128_3_bn1-net2"
  param {
    name: "para1-128_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_3_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_3_scale1-net2"
  type: "Scale"
  bottom: "layer_128_3_bn1-net2"
  top: "layer_128_3_bn1-net2"
  param {
    name: "lamda-128_3_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_3_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_3_relu1-net2"
  type: "ReLU"
  bottom: "layer_128_3_bn1-net2"
  top: "layer_128_3_bn1-net2"
}
layer {
  name: "layer_128_3_conv1-net2"
  type: "Convolution"
  bottom: "layer_128_3_bn1-net2"
  top: "layer_128_3_conv1-net2"
  param {
    name: "weight-128_3_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_3_bn2-net2"
  type: "BatchNorm"
  bottom: "layer_128_3_conv1-net2"
  top: "layer_128_3_conv1-net2"
  param {
    name: "para1-128_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_3_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_3_scale2-net2"
  type: "Scale"
  bottom: "layer_128_3_conv1-net2"
  top: "layer_128_3_conv1-net2"
  param {
    name: "lamda-128_3_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_3_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_3_relu2-net2"
  type: "ReLU"
  bottom: "layer_128_3_conv1-net2"
  top: "layer_128_3_conv1-net2"
}
layer {
  name: "layer_128_3_conv2-net2"
  type: "Convolution"
  bottom: "layer_128_3_conv1-net2"
  top: "layer_128_3_conv2-net2"
  param {
    name: "weight-128_3_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_3_bn3-net2"
  type: "BatchNorm"
  bottom: "layer_128_3_conv2-net2"
  top: "layer_128_3_conv2-net2"
  param {
    name: "para1-128_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_3_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_3_scale3-net2"
  type: "Scale"
  bottom: "layer_128_3_conv2-net2"
  top: "layer_128_3_conv2-net2"
  param {
    name: "lamda-128_3_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_3_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_3_relu3-net2"
  type: "ReLU"
  bottom: "layer_128_3_conv2-net2"
  top: "layer_128_3_conv2-net2"
}
layer {
  name: "layer_128_3_conv3-net2"
  type: "Convolution"
  bottom: "layer_128_3_conv2-net2"
  top: "layer_128_3_conv3-net2"
  param {
    name: "weight-128_3_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_3_sum-net2"
  type: "Eltwise"
  bottom: "layer_128_3_conv3-net2"
  bottom: "layer_128_2_sum-net2"
  top: "layer_128_3_sum-net2"
}
layer {
  name: "layer_128_4_bn1-net2"
  type: "BatchNorm"
  bottom: "layer_128_3_sum-net2"
  top: "layer_128_4_bn1-net2"
  param {
    name: "para1-128_4_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_4_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_4_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_4_scale1-net2"
  type: "Scale"
  bottom: "layer_128_4_bn1-net2"
  top: "layer_128_4_bn1-net2"
  param {
    name: "lamda-128_4_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_4_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_4_relu1-net2"
  type: "ReLU"
  bottom: "layer_128_4_bn1-net2"
  top: "layer_128_4_bn1-net2"
}
layer {
  name: "layer_128_4_conv1-net2"
  type: "Convolution"
  bottom: "layer_128_4_bn1-net2"
  top: "layer_128_4_conv1-net2"
  param {
    name: "weight-128_4_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_4_bn2-net2"
  type: "BatchNorm"
  bottom: "layer_128_4_conv1-net2"
  top: "layer_128_4_conv1-net2"
  param {
    name: "para1-128_4_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_4_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_4_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_4_scale2-net2"
  type: "Scale"
  bottom: "layer_128_4_conv1-net2"
  top: "layer_128_4_conv1-net2"
  param {
    name: "lamda-128_4_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_4_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_4_relu2-net2"
  type: "ReLU"
  bottom: "layer_128_4_conv1-net2"
  top: "layer_128_4_conv1-net2"
}
layer {
  name: "layer_128_4_conv2-net2"
  type: "Convolution"
  bottom: "layer_128_4_conv1-net2"
  top: "layer_128_4_conv2-net2"
  param {
    name: "weight-128_4_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_4_bn3-net2"
  type: "BatchNorm"
  bottom: "layer_128_4_conv2-net2"
  top: "layer_128_4_conv2-net2"
  param {
    name: "para1-128_4_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_4_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_4_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_4_scale3-net2"
  type: "Scale"
  bottom: "layer_128_4_conv2-net2"
  top: "layer_128_4_conv2-net2"
  param {
    name: "lamda-128_4_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_4_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_4_relu3-net2"
  type: "ReLU"
  bottom: "layer_128_4_conv2-net2"
  top: "layer_128_4_conv2-net2"
}
layer {
  name: "layer_128_4_conv3-net2"
  type: "Convolution"
  bottom: "layer_128_4_conv2-net2"
  top: "layer_128_4_conv3-net2"
  param {
    name: "weight-128_4_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_4_sum-net2"
  type: "Eltwise"
  bottom: "layer_128_4_conv3-net2"
  bottom: "layer_128_3_sum-net2"
  top: "layer_128_4_sum-net2"
}
layer {
  name: "layer_256_1_bn1-net2"
  type: "BatchNorm"
  bottom: "layer_128_4_sum-net2"
  top: "layer_256_1_bn1-net2"
  param {
    name: "para1-256_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_1_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_1_scale1-net2"
  type: "Scale"
  bottom: "layer_256_1_bn1-net2"
  top: "layer_256_1_bn1-net2"
  param {
    name: "lamda-256_1_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_1_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_1_relu1-net2"
  type: "ReLU"
  bottom: "layer_256_1_bn1-net2"
  top: "layer_256_1_bn1-net2"
}
layer {
  name: "layer_512_1_bn1-c"
  type: "Convolution"
  bottom: "layer_512_1_bn1"
  top: "layer_512_1_bn1-c"
  param {
    name: "weight-layer_512_1_bn1-c"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_bn1-c-interp"
  type: "Interp"
  bottom: "layer_512_1_bn1-c"
  top: "layer_512_1_bn1-c-interp"
  interp_param {
    height: 48
    width: 48
  }
}
layer {
  name: "conv4-add"
  type: "Eltwise"
  bottom: "layer_256_1_bn1-net2"
  bottom: "layer_512_1_bn1-c-interp"
  top: "conv4-add"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "layer_256_1_conv1-net2"
  type: "Convolution"
  bottom: "conv4-add"
  top: "layer_256_1_conv1-net2"
  param {
    name: "weight-256_1_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_bn2-net2"
  type: "BatchNorm"
  bottom: "layer_256_1_conv1-net2"
  top: "layer_256_1_conv1-net2"
  param {
    name: "para1-256_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_1_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_1_scale2-net2"
  type: "Scale"
  bottom: "layer_256_1_conv1-net2"
  top: "layer_256_1_conv1-net2"
  param {
    name: "lamda-256_1_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_1_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_1_relu2-net2"
  type: "ReLU"
  bottom: "layer_256_1_conv1-net2"
  top: "layer_256_1_conv1-net2"
}
layer {
  name: "layer_256_1_conv2-net2"
  type: "Convolution"
  bottom: "layer_256_1_conv1-net2"
  top: "layer_256_1_conv2-net2"
  param {
    name: "weight-256_1_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_bn3-net2"
  type: "BatchNorm"
  bottom: "layer_256_1_conv2-net2"
  top: "layer_256_1_conv2-net2"
  param {
    name: "para1-256_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_1_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_1_scale3-net2"
  type: "Scale"
  bottom: "layer_256_1_conv2-net2"
  top: "layer_256_1_conv2-net2"
  param {
    name: "lamda-256_1_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_1_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_1_relu3-net2"
  type: "ReLU"
  bottom: "layer_256_1_conv2-net2"
  top: "layer_256_1_conv2-net2"
}
layer {
  name: "layer_256_1_conv3-net2"
  type: "Convolution"
  bottom: "layer_256_1_conv2-net2"
  top: "layer_256_1_conv3-net2"
  param {
    name: "weight-256_1_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_conv_expand-net2"
  type: "Convolution"
  bottom: "conv4-add"
  top: "layer_256_1_conv_expand-net2"
  param {
    name: "weight-256_1_conv_expand"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_sum-net2"
  type: "Eltwise"
  bottom: "layer_256_1_conv3-net2"
  bottom: "layer_256_1_conv_expand-net2"
  top: "layer_256_1_sum-net2"
}

layer {
  name: "layer_256_2_bn1-net2"
  type: "BatchNorm"
  bottom: "layer_256_1_sum-net2"
  top: "layer_256_2_bn1-net2"
  param {
    name: "para1-256_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_2_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_2_scale1-net2"
  type: "Scale"
  bottom: "layer_256_2_bn1-net2"
  top: "layer_256_2_bn1-net2"
  param {
    name: "lamda-256_2_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_2_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_2_relu1-net2"
  type: "ReLU"
  bottom: "layer_256_2_bn1-net2"
  top: "layer_256_2_bn1-net2"
}
layer {
  name: "layer_256_2_conv1-net2"
  type: "Convolution"
  bottom: "layer_256_2_bn1-net2"
  top: "layer_256_2_conv1-net2"
  param {
    name: "weight-256_2_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_2_bn2-net2"
  type: "BatchNorm"
  bottom: "layer_256_2_conv1-net2"
  top: "layer_256_2_conv1-net2"
  param {
    name: "para1-256_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_2_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_2_scale2-net2"
  type: "Scale"
  bottom: "layer_256_2_conv1-net2"
  top: "layer_256_2_conv1-net2"
  param {
    name: "lamda-256_2_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_2_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_2_relu2-net2"
  type: "ReLU"
  bottom: "layer_256_2_conv1-net2"
  top: "layer_256_2_conv1-net2"
}
layer {
  name: "layer_256_2_conv2-net2"
  type: "Convolution"
  bottom: "layer_256_2_conv1-net2"
  top: "layer_256_2_conv2-net2"
  param {
    name: "weight-256_2_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_2_bn3-net2"
  type: "BatchNorm"
  bottom: "layer_256_2_conv2-net2"
  top: "layer_256_2_conv2-net2"
  param {
    name: "para1-256_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_2_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_2_scale3-net2"
  type: "Scale"
  bottom: "layer_256_2_conv2-net2"
  top: "layer_256_2_conv2-net2"
  param {
    name: "lamda-256_2_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_2_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_2_relu3-net2"
  type: "ReLU"
  bottom: "layer_256_2_conv2-net2"
  top: "layer_256_2_conv2-net2"
}
layer {
  name: "layer_256_2_conv3-net2"
  type: "Convolution"
  bottom: "layer_256_2_conv2-net2"
  top: "layer_256_2_conv3-net2"
  param {
    name: "weight-256_2_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_2_sum-net2"
  type: "Eltwise"
  bottom: "layer_256_2_conv3-net2"
  bottom: "layer_256_1_sum-net2"
  top: "layer_256_2_sum-net2"
}
layer {
  name: "layer_256_3_bn1-net2"
  type: "BatchNorm"
  bottom: "layer_256_2_sum-net2"
  top: "layer_256_3_bn1-net2"
  param {
    name: "para1-256_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_3_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_3_scale1-net2"
  type: "Scale"
  bottom: "layer_256_3_bn1-net2"
  top: "layer_256_3_bn1-net2"
  param {
    name: "lamda-256_3_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_3_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_3_relu1-net2"
  type: "ReLU"
  bottom: "layer_256_3_bn1-net2"
  top: "layer_256_3_bn1-net2"
}
layer {
  name: "layer_256_3_conv1-net2"
  type: "Convolution"
  bottom: "layer_256_3_bn1-net2"
  top: "layer_256_3_conv1-net2"
  param {
    name: "weight-256_3_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_3_bn2-net2"
  type: "BatchNorm"
  bottom: "layer_256_3_conv1-net2"
  top: "layer_256_3_conv1-net2"
  param {
    name: "para1-256_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_3_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_3_scale2-net2"
  type: "Scale"
  bottom: "layer_256_3_conv1-net2"
  top: "layer_256_3_conv1-net2"
  param {
    name: "lamda-256_3_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_3_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_3_relu2-net2"
  type: "ReLU"
  bottom: "layer_256_3_conv1-net2"
  top: "layer_256_3_conv1-net2"
}
layer {
  name: "layer_256_3_conv2-net2"
  type: "Convolution"
  bottom: "layer_256_3_conv1-net2"
  top: "layer_256_3_conv2-net2"
  param {
    name: "weight-256_3_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_3_bn3-net2"
  type: "BatchNorm"
  bottom: "layer_256_3_conv2-net2"
  top: "layer_256_3_conv2-net2"
  param {
    name: "para1-256_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_3_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_3_scale3-net2"
  type: "Scale"
  bottom: "layer_256_3_conv2-net2"
  top: "layer_256_3_conv2-net2"
  param {
    name: "lamda-256_3_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_3_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_3_relu3-net2"
  type: "ReLU"
  bottom: "layer_256_3_conv2-net2"
  top: "layer_256_3_conv2-net2"
}
layer {
  name: "layer_256_3_conv3-net2"
  type: "Convolution"
  bottom: "layer_256_3_conv2-net2"
  top: "layer_256_3_conv3-net2"
  param {
    name: "weight-256_3_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_3_sum-net2"
  type: "Eltwise"
  bottom: "layer_256_3_conv3-net2"
  bottom: "layer_256_2_sum-net2"
  top: "layer_256_3_sum-net2"
}
layer {
  name: "layer_256_4_bn1-net2"
  type: "BatchNorm"
  bottom: "layer_256_3_sum-net2"
  top: "layer_256_4_bn1-net2"
  param {
    name: "para1-256_4_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_4_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_4_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_4_scale1-net2"
  type: "Scale"
  bottom: "layer_256_4_bn1-net2"
  top: "layer_256_4_bn1-net2"
  param {
    name: "lamda-256_4_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_4_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_4_relu1-net2"
  type: "ReLU"
  bottom: "layer_256_4_bn1-net2"
  top: "layer_256_4_bn1-net2"
}
layer {
  name: "layer_256_4_conv1-net2"
  type: "Convolution"
  bottom: "layer_256_4_bn1-net2"
  top: "layer_256_4_conv1-net2"
  param {
    name: "weight-256_4_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_4_bn2-net2"
  type: "BatchNorm"
  bottom: "layer_256_4_conv1-net2"
  top: "layer_256_4_conv1-net2"
  param {
    name: "para1-256_4_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_4_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_4_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_4_scale2-net2"
  type: "Scale"
  bottom: "layer_256_4_conv1-net2"
  top: "layer_256_4_conv1-net2"
  param {
    name: "lamda-256_4_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_4_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_4_relu2-net2"
  type: "ReLU"
  bottom: "layer_256_4_conv1-net2"
  top: "layer_256_4_conv1-net2"
}
layer {
  name: "layer_256_4_conv2-net2"
  type: "Convolution"
  bottom: "layer_256_4_conv1-net2"
  top: "layer_256_4_conv2-net2"
  param {
    name: "weight-256_4_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_4_bn3-net2"
  type: "BatchNorm"
  bottom: "layer_256_4_conv2-net2"
  top: "layer_256_4_conv2-net2"
  param {
    name: "para1-256_4_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_4_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_4_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_4_scale3-net2"
  type: "Scale"
  bottom: "layer_256_4_conv2-net2"
  top: "layer_256_4_conv2-net2"
  param {
    name: "lamda-256_4_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_4_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_4_relu3-net2"
  type: "ReLU"
  bottom: "layer_256_4_conv2-net2"
  top: "layer_256_4_conv2-net2"
}
layer {
  name: "layer_256_4_conv3-net2"
  type: "Convolution"
  bottom: "layer_256_4_conv2-net2"
  top: "layer_256_4_conv3-net2"
  param {
    name: "weight-256_4_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_4_sum-net2"
  type: "Eltwise"
  bottom: "layer_256_4_conv3-net2"
  bottom: "layer_256_3_sum-net2"
  top: "layer_256_4_sum-net2"
}
layer {
  name: "layer_256_5_bn1-net2"
  type: "BatchNorm"
  bottom: "layer_256_4_sum-net2"
  top: "layer_256_5_bn1-net2"
  param {
    name: "para1-256_5_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_5_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_5_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_5_scale1-net2"
  type: "Scale"
  bottom: "layer_256_5_bn1-net2"
  top: "layer_256_5_bn1-net2"
  param {
    name: "lamda-256_5_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_5_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_5_relu1-net2"
  type: "ReLU"
  bottom: "layer_256_5_bn1-net2"
  top: "layer_256_5_bn1-net2"
}
layer {
  name: "layer_256_5_conv1-net2"
  type: "Convolution"
  bottom: "layer_256_5_bn1-net2"
  top: "layer_256_5_conv1-net2"
  param {
    name: "weight-256_5_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_5_bn2-net2"
  type: "BatchNorm"
  bottom: "layer_256_5_conv1-net2"
  top: "layer_256_5_conv1-net2"
  param {
    name: "para1-256_5_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_5_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_5_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_5_scale2-net2"
  type: "Scale"
  bottom: "layer_256_5_conv1-net2"
  top: "layer_256_5_conv1-net2"
  param {
    name: "lamda-256_5_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_5_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_5_relu2-net2"
  type: "ReLU"
  bottom: "layer_256_5_conv1-net2"
  top: "layer_256_5_conv1-net2"
}
layer {
  name: "layer_256_5_conv2-net2"
  type: "Convolution"
  bottom: "layer_256_5_conv1-net2"
  top: "layer_256_5_conv2-net2"
  param {
    name: "weight-256_5_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_5_bn3-net2"
  type: "BatchNorm"
  bottom: "layer_256_5_conv2-net2"
  top: "layer_256_5_conv2-net2"
  param {
    name: "para1-256_5_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_5_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_5_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_5_scale3-net2"
  type: "Scale"
  bottom: "layer_256_5_conv2-net2"
  top: "layer_256_5_conv2-net2"
  param {
    name: "lamda-256_5_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_5_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_5_relu3-net2"
  type: "ReLU"
  bottom: "layer_256_5_conv2-net2"
  top: "layer_256_5_conv2-net2"
}
layer {
  name: "layer_256_5_conv3-net2"
  type: "Convolution"
  bottom: "layer_256_5_conv2-net2"
  top: "layer_256_5_conv3-net2"
  param {
    name: "weight-256_5_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_5_sum-net2"
  type: "Eltwise"
  bottom: "layer_256_5_conv3-net2"
  bottom: "layer_256_4_sum-net2"
  top: "layer_256_5_sum-net2"
}
layer {
  name: "layer_256_6_bn1-net2"
  type: "BatchNorm"
  bottom: "layer_256_5_sum-net2"
  top: "layer_256_6_bn1-net2"
  param {
    name: "para1-256_6_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_6_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_6_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_6_scale1-net2"
  type: "Scale"
  bottom: "layer_256_6_bn1-net2"
  top: "layer_256_6_bn1-net2"
  param {
    name: "lamda-256_6_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_6_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_6_relu1-net2"
  type: "ReLU"
  bottom: "layer_256_6_bn1-net2"
  top: "layer_256_6_bn1-net2"
}
layer {
  name: "layer_256_6_conv1-net2"
  type: "Convolution"
  bottom: "layer_256_6_bn1-net2"
  top: "layer_256_6_conv1-net2"
  param {
    name: "weight-256_6_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_6_bn2-net2"
  type: "BatchNorm"
  bottom: "layer_256_6_conv1-net2"
  top: "layer_256_6_conv1-net2"
  param {
    name: "para1-256_6_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_6_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_6_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_6_scale2-net2"
  type: "Scale"
  bottom: "layer_256_6_conv1-net2"
  top: "layer_256_6_conv1-net2"
  param {
    name: "lamda-256_6_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_6_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_6_relu2-net2"
  type: "ReLU"
  bottom: "layer_256_6_conv1-net2"
  top: "layer_256_6_conv1-net2"
}
layer {
  name: "layer_256_6_conv2-net2"
  type: "Convolution"
  bottom: "layer_256_6_conv1-net2"
  top: "layer_256_6_conv2-net2"
  param {
    name: "weight-256_6_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_6_bn3-net2"
  type: "BatchNorm"
  bottom: "layer_256_6_conv2-net2"
  top: "layer_256_6_conv2-net2"
  param {
    name: "para1-256_6_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_6_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_6_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_6_scale3-net2"
  type: "Scale"
  bottom: "layer_256_6_conv2-net2"
  top: "layer_256_6_conv2-net2"
  param {
    name: "lamda-256_6_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_6_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_6_relu3-net2"
  type: "ReLU"
  bottom: "layer_256_6_conv2-net2"
  top: "layer_256_6_conv2-net2"
}
layer {
  name: "layer_256_6_conv3-net2"
  type: "Convolution"
  bottom: "layer_256_6_conv2-net2"
  top: "layer_256_6_conv3-net2"
  param {
    name: "weight-256_6_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_6_sum-net2"
  type: "Eltwise"
  bottom: "layer_256_6_conv3-net2"
  bottom: "layer_256_5_sum-net2"
  top: "layer_256_6_sum-net2"
}
layer {
  name: "layer_512_1_bn1-net2"
  type: "BatchNorm"
  bottom: "layer_256_6_sum-net2"
  top: "layer_512_1_bn1-net2"
  param {
    name: "para1-512_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_1_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_1_scale1-net2"
  type: "Scale"
  bottom: "layer_512_1_bn1-net2"
  top: "layer_512_1_bn1-net2"
  param {
    name: "lamda-512_1_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_1_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_1_relu1-net2"
  type: "ReLU"
  bottom: "layer_512_1_bn1-net2"
  top: "layer_512_1_bn1-net2"
}

layer {
  name: "layer_512_3_sum-c"
  type: "Convolution"
  bottom: "layer_512_3_sum"
  top: "layer_512_3_sum-c"
  param {
    name: "weight-layer_512_3_sum-c"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_3_sum-c-interp"
  type: "Interp"
  bottom: "layer_512_3_sum-c"
  top: "layer_512_3_sum-c-interp"
  interp_param {
    height: 24
    width: 24
  }
}
layer {
  name: "conv5-add"
  type: "Eltwise"
  bottom: "layer_512_1_bn1-net2"
  bottom: "layer_512_3_sum-c-interp"
  top: "conv5-add"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "layer_512_1_conv1-net2"
  type: "Convolution"
  bottom: "conv5-add"
  top: "layer_512_1_conv1-net2"
  param {
    name: "weight-512_1_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_bn2-net2"
  type: "BatchNorm"
  bottom: "layer_512_1_conv1-net2"
  top: "layer_512_1_conv1-net2"
  param {
    name: "para1-512_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_1_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_1_scale2-net2"
  type: "Scale"
  bottom: "layer_512_1_conv1-net2"
  top: "layer_512_1_conv1-net2"
  param {
    name: "lamda-512_1_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_1_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_1_relu2-net2"
  type: "ReLU"
  bottom: "layer_512_1_conv1-net2"
  top: "layer_512_1_conv1-net2"
}
layer {
  name: "layer_512_1_conv2-net2"
  type: "Convolution"
  bottom: "layer_512_1_conv1-net2"
  top: "layer_512_1_conv2-net2"
  param {
    name: "weight-512_1_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_bn3-net2"
  type: "BatchNorm"
  bottom: "layer_512_1_conv2-net2"
  top: "layer_512_1_conv2-net2"
  param {
    name: "para1-512_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_1_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_1_scale3-net2"
  type: "Scale"
  bottom: "layer_512_1_conv2-net2"
  top: "layer_512_1_conv2-net2"
  param {
    name: "lamda-512_1_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_1_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_1_relu3-net2"
  type: "ReLU"
  bottom: "layer_512_1_conv2-net2"
  top: "layer_512_1_conv2-net2"
}
layer {
  name: "layer_512_1_conv3-net2"
  type: "Convolution"
  bottom: "layer_512_1_conv2-net2"
  top: "layer_512_1_conv3-net2"
  param {
    name: "weight-512_1_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 2048
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_conv_expand-net2"
  type: "Convolution"
  bottom: "conv5-add"
  top: "layer_512_1_conv_expand-net2"
  param {
    name: "weight-512_1_conv_expand"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 2048
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_sum-net2"
  type: "Eltwise"
  bottom: "layer_512_1_conv3-net2"
  bottom: "layer_512_1_conv_expand-net2"
  top: "layer_512_1_sum-net2"
}

layer {
  name: "layer_512_2_bn1-net2"
  type: "BatchNorm"
  bottom: "layer_512_1_sum-net2"
  top: "layer_512_2_bn1-net2"
  param {
    name: "para1-512_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_2_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_2_scale1-net2"
  type: "Scale"
  bottom: "layer_512_2_bn1-net2"
  top: "layer_512_2_bn1-net2"
  param {
    name: "lamda-512_2_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_2_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_2_relu1-net2"
  type: "ReLU"
  bottom: "layer_512_2_bn1-net2"
  top: "layer_512_2_bn1-net2"
}
layer {
  name: "layer_512_2_conv1-net2"
  type: "Convolution"
  bottom: "layer_512_2_bn1-net2"
  top: "layer_512_2_conv1-net2"
  param {
    name: "weight-512_2_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_2_bn2-net2"
  type: "BatchNorm"
  bottom: "layer_512_2_conv1-net2"
  top: "layer_512_2_conv1-net2"
  param {
    name: "para1-512_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_2_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_2_scale2-net2"
  type: "Scale"
  bottom: "layer_512_2_conv1-net2"
  top: "layer_512_2_conv1-net2"
  param {
    name: "lamda-512_2_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_2_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_2_relu2-net2"
  type: "ReLU"
  bottom: "layer_512_2_conv1-net2"
  top: "layer_512_2_conv1-net2"
}
layer {
  name: "layer_512_2_conv2-net2"
  type: "Convolution"
  bottom: "layer_512_2_conv1-net2"
  top: "layer_512_2_conv2-net2"
  param {
    name: "weight-512_2_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_2_bn3-net2"
  type: "BatchNorm"
  bottom: "layer_512_2_conv2-net2"
  top: "layer_512_2_conv2-net2"
  param {
    name: "para1-512_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_2_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_2_scale3-net2"
  type: "Scale"
  bottom: "layer_512_2_conv2-net2"
  top: "layer_512_2_conv2-net2"
  param {
    name: "lamda-512_2_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_2_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_2_relu3-net2"
  type: "ReLU"
  bottom: "layer_512_2_conv2-net2"
  top: "layer_512_2_conv2-net2"
}
layer {
  name: "layer_512_2_conv3-net2"
  type: "Convolution"
  bottom: "layer_512_2_conv2-net2"
  top: "layer_512_2_conv3-net2"
  param {
    name: "weight-512_2_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 2048
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_2_sum-net2"
  type: "Eltwise"
  bottom: "layer_512_2_conv3-net2"
  bottom: "layer_512_1_sum-net2"
  top: "layer_512_2_sum-net2"
}
layer {
  name: "layer_512_3_bn1-net2"
  type: "BatchNorm"
  bottom: "layer_512_2_sum-net2"
  top: "layer_512_3_bn1-net2"
  param {
    name: "para1-512_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_3_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_3_scale1-net2"
  type: "Scale"
  bottom: "layer_512_3_bn1-net2"
  top: "layer_512_3_bn1-net2"
  param {
    name: "lamda-512_3_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_3_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_3_relu1-net2"
  type: "ReLU"
  bottom: "layer_512_3_bn1-net2"
  top: "layer_512_3_bn1-net2"
}
layer {
  name: "layer_512_3_conv1-net2"
  type: "Convolution"
  bottom: "layer_512_3_bn1-net2"
  top: "layer_512_3_conv1-net2"
  param {
    name: "weight-512_3_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_3_bn2-net2"
  type: "BatchNorm"
  bottom: "layer_512_3_conv1-net2"
  top: "layer_512_3_conv1-net2"
  param {
    name: "para1-512_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_3_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_3_scale2-net2"
  type: "Scale"
  bottom: "layer_512_3_conv1-net2"
  top: "layer_512_3_conv1-net2"
  param {
    name: "lamda-512_3_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_3_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_3_relu2-net2"
  type: "ReLU"
  bottom: "layer_512_3_conv1-net2"
  top: "layer_512_3_conv1-net2"
}
layer {
  name: "layer_512_3_conv2-net2"
  type: "Convolution"
  bottom: "layer_512_3_conv1-net2"
  top: "layer_512_3_conv2-net2"
  param {
    name: "weight-512_3_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_3_bn3-net2"
  type: "BatchNorm"
  bottom: "layer_512_3_conv2-net2"
  top: "layer_512_3_conv2-net2"
  param {
    name: "para1-512_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_3_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_3_scale3-net2"
  type: "Scale"
  bottom: "layer_512_3_conv2-net2"
  top: "layer_512_3_conv2-net2"
  param {
    name: "lamda-512_3_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_3_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_3_relu3-net2"
  type: "ReLU"
  bottom: "layer_512_3_conv2-net2"
  top: "layer_512_3_conv2-net2"
}
layer {
  name: "layer_512_3_conv3-net2"
  type: "Convolution"
  bottom: "layer_512_3_conv2-net2"
  top: "layer_512_3_conv3-net2"
  param {
    name: "weight-512_3_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 2048
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_3_sum-net2"
  type: "Eltwise"
  bottom: "layer_512_3_conv3-net2"
  bottom: "layer_512_2_sum-net2"
  top: "layer_512_3_sum-net2"
}
layer {
  name: "last_bn-net2"
  type: "BatchNorm"
  bottom: "layer_512_3_sum-net2"
  top: "layer_512_3_sum-net2"
  param {
    name: "para1-last_bn"
    lr_mult: 0.0
  }
  param {
    name: "para2-last_bn"
    lr_mult: 0.0
  }
  param {
    name: "para3-last_bn"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "last_scale-net2"
  type: "Scale"
  bottom: "layer_512_3_sum-net2"
  top: "layer_512_3_sum-net2"
  param {
    name: "lamda-last_scale"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-last_scale"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "last_relu-net2"
  type: "ReLU"
  bottom: "layer_512_3_sum-net2"
  top: "layer_512_3_sum-net2"
}
#
# conv5 output
#
layer {
  name: "conv6-net2"
  type: "Convolution"
  bottom: "layer_512_3_sum-net2"
  top: "conv6-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
# 
# spatial attention
#
layer {
  name: "conv5_4/bn-pool1-net2"
  type: "Interp"
  bottom: "layer_512_3_sum-net2"
  top: "conv5_4/bn-pool1-net2"
  interp_param {
    height: 6
    width: 6
  }
}

layer {
  name: "spa.att_Conv_1-net2"
  type: "Convolution"
  bottom: "conv5_4/bn-pool1-net2"
  top: "spa.att_Conv_1-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_1-norm-net2"
  type: "Normalize"
  bottom: "spa.att_Conv_1-net2"
  top: "spa.att_Conv_1-norm-net2"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_2-net2"
  type: "Convolution"
  bottom: "conv5_4/bn-pool1-net2"
  top: "spa.att_Conv_2-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_2-norm-net2"
  type: "Normalize"
  bottom: "spa.att_Conv_2-net2"
  top: "spa.att_Conv_2-norm-net2"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_3-net2"
  type: "Convolution"
  bottom: "conv5_4/bn-pool1-net2"
  top: "spa.att_Conv_3-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_3-norm-net2"
  type: "Normalize"
  bottom: "spa.att_Conv_3-net2"
  top: "spa.att_Conv_3-norm-net2"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_all-norm-net2"
  type: "Concat"
  bottom: "spa.att_Conv_1-norm-net2"
  bottom: "spa.att_Conv_2-norm-net2"
  bottom: "spa.att_Conv_3-norm-net2"
  top: "spa.att_all-norm-net2"
}
layer {
  name: "spa.att_all-norm_mask-net2"
  type: "Convolution"
  bottom: "spa.att_all-norm-net2"
  top: "spa.att_all-norm_mask-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 1
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Reshape_4-net2"
  type: "Reshape"
  bottom: "spa.att_all-norm_mask-net2"
  top: "spa.att_Reshape_4-net2"
  reshape_param {
    shape {
      dim: -1
      dim: 36
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "spa.att_softmax_5-net2"
  type: "Softmax"
  bottom: "spa.att_Reshape_4-net2"
  top: "spa.att_softmax_5-net2"
  softmax_param {
    axis: 1
  }
}
layer {
  name: "spa.att_Reshape_6-net2"
  type: "Reshape"
  bottom: "spa.att_softmax_5-net2"
  top: "spa.att_Reshape_6-net2"
  reshape_param {
    shape {
      dim: -1
      dim: 1
      dim: 6
      dim: 6
    }
  }
}
layer {
  name: "spa.att_Reshape_6-power-net2"
  type: "Power"
  bottom: "spa.att_Reshape_6-net2"
  top: "spa.att_Reshape_6-power-net2"
  power_param {
    power: 1
    scale: 36
    shift: 0
  }
}
layer {
  name: "spa.att_Reshape_6-power_interp-net2"
  type: "Interp"
  bottom: "spa.att_Reshape_6-power-net2"
  top: "spa.att_Reshape_6-power_interp-net2"
  interp_param {
    height: 12
    width: 12
  }
}
layer {
  name: "spa.att_Reshape_6-power_tile-net2"
  type: "Tile"
  bottom: "spa.att_Reshape_6-power_interp-net2"
  top: "spa.att_Reshape_6-power_tile-net2"
  tile_param {
    tiles: 128
  }
}
# 
# spatial end
#
layer {
  name: "conv6_prod-net2"
  type: "Eltwise"
  bottom: "conv6-net2"
  bottom: "spa.att_Reshape_6-power_tile-net2"
  top: "conv6_prod-net2"
  eltwise_param {
    operation: PROD
  }
}

#

#
# conv4 output
#
layer {
  name: "conv6-c4-net2"
  type: "Convolution"
  bottom: "layer_512_1_bn1-net2"
  top: "conv6-c4-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
# 
# spatial attention
#
layer {
  name: "conv4_4/bn-pool1-c4-net2"
  type: "Interp"
  bottom: "layer_512_1_bn1-net2"
  top: "conv4_4/bn-pool1-c4-net2"
  interp_param {
    height: 12
    width: 12
  }
}

layer {
  name: "spa.att_Conv_1-c4-net2"
  type: "Convolution"
  bottom: "conv4_4/bn-pool1-c4-net2"
  top: "spa.att_Conv_1-c4-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_1-norm-c4-net2"
  type: "Normalize"
  bottom: "spa.att_Conv_1-c4-net2"
  top: "spa.att_Conv_1-norm-c4-net2"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_2-c4-net2"
  type: "Convolution"
  bottom: "conv4_4/bn-pool1-c4-net2"
  top: "spa.att_Conv_2-c4-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_2-norm-c4-net2"
  type: "Normalize"
  bottom: "spa.att_Conv_2-c4-net2"
  top: "spa.att_Conv_2-norm-c4-net2"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_3-c4-net2"
  type: "Convolution"
  bottom: "conv4_4/bn-pool1-c4-net2"
  top: "spa.att_Conv_3-c4-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_3-norm-c4-net2"
  type: "Normalize"
  bottom: "spa.att_Conv_3-c4-net2"
  top: "spa.att_Conv_3-norm-c4-net2"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_all-norm-c4-net2"
  type: "Concat"
  bottom: "spa.att_Conv_1-norm-c4-net2"
  bottom: "spa.att_Conv_2-norm-c4-net2"
  bottom: "spa.att_Conv_3-norm-c4-net2"
  top: "spa.att_all-norm-c4-net2"
}
layer {
  name: "spa.att_all-norm_mask-c4-net2"
  type: "Convolution"
  bottom: "spa.att_all-norm-c4-net2"
  top: "spa.att_all-norm_mask-c4-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 1
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Reshape_4-c4-net2"
  type: "Reshape"
  bottom: "spa.att_all-norm_mask-c4-net2"
  top: "spa.att_Reshape_4-c4-net2"
  reshape_param {
    shape {
      dim: -1
      dim: 144
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "spa.att_softmax_5-c4-net2"
  type: "Softmax"
  bottom: "spa.att_Reshape_4-c4-net2"
  top: "spa.att_softmax_5-c4-net2"
  softmax_param {
    axis: 1
  }
}
layer {
  name: "spa.att_Reshape_6-c4-net2"
  type: "Reshape"
  bottom: "spa.att_softmax_5-c4-net2"
  top: "spa.att_Reshape_6-c4-net2"
  reshape_param {
    shape {
      dim: -1
      dim: 1
      dim: 12
      dim: 12
    }
  }
}
layer {
  name: "spa.att_Reshape_6-power-c4-net2"
  type: "Power"
  bottom: "spa.att_Reshape_6-c4-net2"
  top: "spa.att_Reshape_6-power-c4-net2"
  power_param {
    power: 1
    scale: 144
    shift: 0
  }
}
layer {
  name: "spa.att_Reshape_6-power_interp-c4-net2"
  type: "Interp"
  bottom: "spa.att_Reshape_6-power-c4-net2"
  top: "spa.att_Reshape_6-power_interp-c4-net2"
  interp_param {
    height: 24
    width: 24
  }
}
layer {
  name: "spa.att_Reshape_6-power_tile-c4-net2"
  type: "Tile"
  bottom: "spa.att_Reshape_6-power_interp-c4-net2"
  top: "spa.att_Reshape_6-power_tile-c4-net2"
  tile_param {
    tiles: 128
  }
}
# 
# spatial end
#
layer {
  name: "conv6_prod-c4-net2"
  type: "Eltwise"
  bottom: "conv6-c4-net2"
  bottom: "spa.att_Reshape_6-power_tile-c4-net2"
  top: "conv6_prod-c4-net2"
  eltwise_param {
    operation: PROD
  }
}
#
#
#
#
# conv3 output
#
layer {
  name: "conv6-c3-net2"
  type: "Convolution"
  bottom: "layer_256_1_bn1-net2"
  top: "conv6-c3-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
# 
# spatial attention
#
layer {
  name: "conv3_4/bn-pool1-c3-net2"
  type: "Interp"
  bottom: "layer_256_1_bn1-net2"
  top: "conv3_4/bn-pool1-c3-net2"
  interp_param {
    height: 24
    width: 24
  }
}

layer {
  name: "spa.att_Conv_1-c3-net2"
  type: "Convolution"
  bottom: "conv3_4/bn-pool1-c3-net2"
  top: "spa.att_Conv_1-c3-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_1-norm-c3-net2"
  type: "Normalize"
  bottom: "spa.att_Conv_1-c3-net2"
  top: "spa.att_Conv_1-norm-c3-net2"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_2-c3-net2"
  type: "Convolution"
  bottom: "conv3_4/bn-pool1-c3-net2"
  top: "spa.att_Conv_2-c3-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_2-norm-c3-net2"
  type: "Normalize"
  bottom: "spa.att_Conv_2-c3-net2"
  top: "spa.att_Conv_2-norm-c3-net2"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_3-c3-net2"
  type: "Convolution"
  bottom: "conv3_4/bn-pool1-c3-net2"
  top: "spa.att_Conv_3-c3-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_3-norm-c3-net2"
  type: "Normalize"
  bottom: "spa.att_Conv_3-c3-net2"
  top: "spa.att_Conv_3-norm-c3-net2"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_all-norm-c3-net2"
  type: "Concat"
  bottom: "spa.att_Conv_1-norm-c3-net2"
  bottom: "spa.att_Conv_2-norm-c3-net2"
  bottom: "spa.att_Conv_3-norm-c3-net2"
  top: "spa.att_all-norm-c3-net2"
}
layer {
  name: "spa.att_all-norm_mask-c3-net2"
  type: "Convolution"
  bottom: "spa.att_all-norm-c3-net2"
  top: "spa.att_all-norm_mask-c3-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 1
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Reshape_4-c3-net2"
  type: "Reshape"
  bottom: "spa.att_all-norm_mask-c3-net2"
  top: "spa.att_Reshape_4-c3-net2"
  reshape_param {
    shape {
      dim: -1
      dim: 576
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "spa.att_softmax_5-c3-net2"
  type: "Softmax"
  bottom: "spa.att_Reshape_4-c3-net2"
  top: "spa.att_softmax_5-c3-net2"
  softmax_param {
    axis: 1
  }
}
layer {
  name: "spa.att_Reshape_6-c3-net2"
  type: "Reshape"
  bottom: "spa.att_softmax_5-c3-net2"
  top: "spa.att_Reshape_6-c3-net2"
  reshape_param {
    shape {
      dim: -1
      dim: 1
      dim: 24
      dim: 24
    }
  }
}
layer {
  name: "spa.att_Reshape_6-power-c3-net2"
  type: "Power"
  bottom: "spa.att_Reshape_6-c3-net2"
  top: "spa.att_Reshape_6-power-c3-net2"
  power_param {
    power: 1
    scale: 576
    shift: 0
  }
}
layer {
  name: "spa.att_Reshape_6-power_interp-c3-net2"
  type: "Interp"
  bottom: "spa.att_Reshape_6-power-c3-net2"
  top: "spa.att_Reshape_6-power_interp-c3-net2"
  interp_param {
    height: 48
    width: 48
  }
}
layer {
  name: "spa.att_Reshape_6-power_tile-c3-net2"
  type: "Tile"
  bottom: "spa.att_Reshape_6-power_interp-c3-net2"
  top: "spa.att_Reshape_6-power_tile-c3-net2"
  tile_param {
    tiles: 128
  }
}
# 
# spatial end
#
layer {
  name: "conv6_prod-c3-net2"
  type: "Eltwise"
  bottom: "conv6-c3-net2"
  bottom: "spa.att_Reshape_6-power_tile-c3-net2"
  top: "conv6_prod-c3-net2"
  eltwise_param {
    operation: PROD
  }
}
#
#
#
# three scale concat
#
layer {
  name: "conv6_prod_interp-net2"
  type: "Interp"
  bottom: "conv6_prod-net2"
  top: "conv6_prod_interp-net2"
  interp_param {
    height: 48
    width: 48
  }
}
layer {
  name: "conv6_prod-c4_interp-net2"
  type: "Interp"
  bottom: "conv6_prod-c4-net2"
  top: "conv6_prod-c4_interp-net2"
  interp_param {
    height: 48
    width: 48
  }
}

layer {
  name: "conv6_prod-concat-net2"
  type: "Eltwise"
  bottom: "conv6_prod-c3-net2"
  bottom: "conv6_prod-c4_interp-net2"
  bottom: "conv6_prod_interp-net2"
  top: "conv6_prod-concat-net2"

}
layer {
  name: "conv7-net2"
  type: "Convolution"
  bottom: "conv6_prod-concat-net2"
  top: "conv7-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "conv7-2-net2"
  type: "Convolution"
  bottom: "conv7-net2"
  top: "conv7-2-net2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  convolution_param {
    num_output: 2
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "conv7_interp-net2"
  type: "Interp"
  bottom: "conv7-2-net2"
  top: "conv7_interp-net2"
  interp_param {
    height: 384
    width: 384
  }
}
layer {
  name: "loss-net2"
  type: "SoftmaxWithLoss"
  bottom: "conv7_interp-net2"
  bottom: "label-net2"
  top: "loss-net2"
  loss_param {
#    ignore_label: 255
    normalize: false
  }
}
#
#

#
# net3
#
layer {
  name: "data-net3"
  type: "ImageData"
  top: "data-net3"
  top: "label_img1-net3"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_value: 0.0
    mean_value: 0.0
    mean_value: 0.0
  }
  image_data_param {
    source: "/media/wtt/Data/datasets/train_duts.txt"
    batch_size: 1
    new_height: 384
    new_width: 384
  }
}
layer {
  name: "label-net3"
  type: "ImageData"
  top: "label-net3"
  top: "label_gt-net3"
  include {
    phase: TRAIN
  }
  image_data_param {
    source: "/media/wtt/Data/datasets/train_duts_gt01.txt"
    is_color: false
    batch_size: 1
    new_height: 384
    new_width: 384
  }
}
layer {
  name: "label_img1-net3"
  type: "Silence"
  bottom: "label_img1-net3"
}
layer {
  name: "label_gt-net3"
  type: "Silence"
  bottom: "label_gt-net3"
}
layer {
  name: "data_bn-net3"
  type: "BatchNorm"
  bottom: "data-net3"
  top: "data_bn-net3"
  param {
    name: "para1-data"
    lr_mult: 0.0
  }
  param {
    name: "para2-data"
    lr_mult: 0.0
  }
  param {
    name: "para3-data"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "data_scale-net3"
  type: "Scale"
  bottom: "data_bn-net3"
  top: "data_bn-net3"
  param {
    name: "lamda-data"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-data"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}

layer {
  name: "conv1-net3"
  type: "Convolution"
  bottom: "data_bn-net3"
  top: "conv1-net3"
  param {
    name: "weight-c1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "bias-c1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    pad: 3
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "msra"
      variance_norm: FAN_OUT
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "conv1_bn-net3"
  type: "BatchNorm"
  bottom: "conv1-net3"
  top: "conv1-net3"
  param {
    name: "para1-c1"
    lr_mult: 0.0
  }
  param {
    name: "para2-c1"
    lr_mult: 0.0
  }
  param {
    name: "para3-c1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv1_scale-net3"
  type: "Scale"
  bottom: "conv1-net3"
  top: "conv1-net3"
  param {
    name: "lamda-c1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-c1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1_relu-net3"
  type: "ReLU"
  bottom: "conv1-net3"
  top: "conv1-net3"
}
layer {
  name: "layer_128_1_bn1-c-net2"
  type: "Convolution"
  bottom: "layer_128_1_bn1-net2"
  top: "layer_128_1_bn1-c-net2"
  param {
    name: "weight-layer_128_1_bn1-c"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_bn1-interp-net2"
  type: "Interp"
  bottom: "layer_128_1_bn1-c-net2"
  top: "layer_128_1_bn1-c-interp-net2"
  interp_param {
    height: 192
    width: 192
  }
}
layer {
  name: "conv2-add-net2"
  type: "Eltwise"
  bottom: "conv1-net3"
  bottom: "layer_128_1_bn1-c-interp-net2"
  top: "conv2-add-net2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv1_pool-net3"
  type: "Pooling"
  bottom: "conv2-add-net2"
  top: "conv1_pool-net3"
  pooling_param {
    kernel_size: 3
    stride: 2
    pad: 0
  }
}

layer {
  name: "layer_64_1_conv1-net3"
  type: "Convolution"
  bottom: "conv1_pool-net3"
  top: "layer_64_1_conv1-net3"
  param {
    name: "weight-64_1_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_1_bn2-net3"
  type: "BatchNorm"
  bottom: "layer_64_1_conv1-net3"
  top: "layer_64_1_conv1-net3"
  param {
    name: "para1-64_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_1_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_1_scale2-net3"
  type: "Scale"
  bottom: "layer_64_1_conv1-net3"
  top: "layer_64_1_conv1-net3"
  param {
    name: "lamda-64_1_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_1_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_1_relu2-net3"
  type: "ReLU"
  bottom: "layer_64_1_conv1-net3"
  top: "layer_64_1_conv1-net3"
}
layer {
  name: "layer_64_1_conv2-net3"
  type: "Convolution"
  bottom: "layer_64_1_conv1-net3"
  top: "layer_64_1_conv2-net3"
  param {
    name: "weight-64_1_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_1_bn3-net3"
  type: "BatchNorm"
  bottom: "layer_64_1_conv2-net3"
  top: "layer_64_1_conv2-net3"
  param {
    name: "para1-64_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_1_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_1_scale3-net3"
  type: "Scale"
  bottom: "layer_64_1_conv2-net3"
  top: "layer_64_1_conv2-net3"
  param {
    name: "lamda-64_1_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_1_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_1_relu3-net3"
  type: "ReLU"
  bottom: "layer_64_1_conv2-net3"
  top: "layer_64_1_conv2-net3"
}
layer {
  name: "layer_64_1_conv3-net3"
  type: "Convolution"
  bottom: "layer_64_1_conv2-net3"
  top: "layer_64_1_conv3-net3"
  param {
    name: "weight-64_1_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_1_conv_expand-net3"
  type: "Convolution"
  bottom: "layer_64_1_conv1-net3"
  top: "layer_64_1_conv_expand-net3"
  param {
    name: "weight-64_1_conv_expand"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_1_sum-net3"
  type: "Eltwise"
  bottom: "layer_64_1_conv3-net3"
  bottom: "layer_64_1_conv_expand-net3"
  top: "layer_64_1_sum-net3"
}
layer {
  name: "layer_64_2_bn1-net3"
  type: "BatchNorm"
  bottom: "layer_64_1_sum-net3"
  top: "layer_64_2_bn1-net3"
  param {
    name: "para1-64_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_2_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_2_scale1-net3"
  type: "Scale"
  bottom: "layer_64_2_bn1-net3"
  top: "layer_64_2_bn1-net3"
  param {
    name: "lamda-64_2_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_2_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_2_relu1-net3"
  type: "ReLU"
  bottom: "layer_64_2_bn1-net3"
  top: "layer_64_2_bn1-net3"
}
layer {
  name: "layer_64_2_conv1-net3"
  type: "Convolution"
  bottom: "layer_64_2_bn1-net3"
  top: "layer_64_2_conv1-net3"
  param {
    name: "weight-64_2_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_2_bn2-net3"
  type: "BatchNorm"
  bottom: "layer_64_2_conv1-net3"
  top: "layer_64_2_conv1-net3"
  param {
    name: "para1-64_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_2_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_2_scale2-net3"
  type: "Scale"
  bottom: "layer_64_2_conv1-net3"
  top: "layer_64_2_conv1-net3"
  param {
    name: "lamda-64_2_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_2_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_2_relu2-net3"
  type: "ReLU"
  bottom: "layer_64_2_conv1-net3"
  top: "layer_64_2_conv1-net3"
}
layer {
  name: "layer_64_2_conv2-net3"
  type: "Convolution"
  bottom: "layer_64_2_conv1-net3"
  top: "layer_64_2_conv2-net3"
  param {
    name: "weight-64_2_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_2_bn3-net3"
  type: "BatchNorm"
  bottom: "layer_64_2_conv2-net3"
  top: "layer_64_2_conv2-net3"
  param {
    name: "para1-64_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_2_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_2_scale3-net3"
  type: "Scale"
  bottom: "layer_64_2_conv2-net3"
  top: "layer_64_2_conv2-net3"
  param {
    name: "lamda-64_2_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_2_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_2_relu3-net3"
  type: "ReLU"
  bottom: "layer_64_2_conv2-net3"
  top: "layer_64_2_conv2-net3"
}
layer {
  name: "layer_64_2_conv3-net3"
  type: "Convolution"
  bottom: "layer_64_2_conv2-net3"
  top: "layer_64_2_conv3-net3"
  param {
    name: "weight-64_2_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_2_sum-net3"
  type: "Eltwise"
  bottom: "layer_64_2_conv3-net3"
  bottom: "layer_64_1_sum-net3"
  top: "layer_64_2_sum-net3"
}
layer {
  name: "layer_64_3_bn1-net3"
  type: "BatchNorm"
  bottom: "layer_64_2_sum-net3"
  top: "layer_64_3_bn1-net3"
  param {
    name: "para1-64_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_3_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_3_scale1-net3"
  type: "Scale"
  bottom: "layer_64_3_bn1-net3"
  top: "layer_64_3_bn1-net3"
  param {
    name: "lamda-64_3_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_3_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_3_relu1-net3"
  type: "ReLU"
  bottom: "layer_64_3_bn1-net3"
  top: "layer_64_3_bn1-net3"
}
layer {
  name: "layer_64_3_conv1-net3"
  type: "Convolution"
  bottom: "layer_64_3_bn1-net3"
  top: "layer_64_3_conv1-net3"
  param {
    name: "weight-64_3_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_3_bn2-net3"
  type: "BatchNorm"
  bottom: "layer_64_3_conv1-net3"
  top: "layer_64_3_conv1-net3"
  param {
    name: "para1-64_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_3_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_3_scale2-net3"
  type: "Scale"
  bottom: "layer_64_3_conv1-net3"
  top: "layer_64_3_conv1-net3"
  param {
    name: "lamda-64_3_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_3_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_3_relu2-net3"
  type: "ReLU"
  bottom: "layer_64_3_conv1-net3"
  top: "layer_64_3_conv1-net3"
}
layer {
  name: "layer_64_3_conv2-net3"
  type: "Convolution"
  bottom: "layer_64_3_conv1-net3"
  top: "layer_64_3_conv2-net3"
  param {
    name: "weight-64_3_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_3_bn3-net3"
  type: "BatchNorm"
  bottom: "layer_64_3_conv2-net3"
  top: "layer_64_3_conv2-net3"
  param {
    name: "para1-64_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-64_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-64_3_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_64_3_scale3-net3"
  type: "Scale"
  bottom: "layer_64_3_conv2-net3"
  top: "layer_64_3_conv2-net3"
  param {
    name: "lamda-64_3_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-64_3_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_3_relu3-net3"
  type: "ReLU"
  bottom: "layer_64_3_conv2-net3"
  top: "layer_64_3_conv2-net3"
}
layer {
  name: "layer_64_3_conv3-net3"
  type: "Convolution"
  bottom: "layer_64_3_conv2-net3"
  top: "layer_64_3_conv3-net3"
  param {
    name: "weight-64_3_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_3_sum-net3"
  type: "Eltwise"
  bottom: "layer_64_3_conv3-net3"
  bottom: "layer_64_2_sum-net3"
  top: "layer_64_3_sum-net3"
}
layer {
  name: "layer_128_1_bn1-net3"
  type: "BatchNorm"
  bottom: "layer_64_3_sum-net3"
  top: "layer_128_1_bn1-net3"
  param {
    name: "para1-128_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_1_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_1_scale1-net3"
  type: "Scale"
  bottom: "layer_128_1_bn1-net3"
  top: "layer_128_1_bn1-net3"
  param {
    name: "lamda-128_1_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_1_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_1_relu1-net3"
  type: "ReLU"
  bottom: "layer_128_1_bn1-net3"
  top: "layer_128_1_bn1-net3"
}
layer {
  name: "layer_256_1_bn1-c-net2"
  type: "Convolution"
  bottom: "layer_256_1_bn1-net2"
  top: "layer_256_1_bn1-c-net2"
  param {
    name: "weight-layer_256_1_bn1-c"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_bn1-c-interp-net2"
  type: "Interp"
  bottom: "layer_256_1_bn1-c-net2"
  top: "layer_256_1_bn1-c-interp-net2"
  interp_param {
    height: 96
    width: 96
  }
}
layer {
  name: "conv3-add-net2"
  type: "Eltwise"
  bottom: "layer_128_1_bn1-net3"
  bottom: "layer_256_1_bn1-c-interp-net2"
  top: "conv3-add-net2"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "layer_128_1_conv1-net3"
  type: "Convolution"
  bottom: "conv3-add-net2"
  top: "layer_128_1_conv1-net3"
  param {
    name: "weight-128_1_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_bn2-net3"
  type: "BatchNorm"
  bottom: "layer_128_1_conv1-net3"
  top: "layer_128_1_conv1-net3"
  param {
    name: "para1-128_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_1_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_1_scale2-net3"
  type: "Scale"
  bottom: "layer_128_1_conv1-net3"
  top: "layer_128_1_conv1-net3"
  param {
    name: "lamda-128_1_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_1_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_1_relu2-net3"
  type: "ReLU"
  bottom: "layer_128_1_conv1-net3"
  top: "layer_128_1_conv1-net3"
}
layer {
  name: "layer_128_1_conv2-net3"
  type: "Convolution"
  bottom: "layer_128_1_conv1-net3"
  top: "layer_128_1_conv2-net3"
  param {
    name: "weight-128_1_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_bn3-net3"
  type: "BatchNorm"
  bottom: "layer_128_1_conv2-net3"
  top: "layer_128_1_conv2-net3"
  param {
    name: "para1-128_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_1_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_1_scale3-net3"
  type: "Scale"
  bottom: "layer_128_1_conv2-net3"
  top: "layer_128_1_conv2-net3"
  param {
    name: "lamda-128_1_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_1_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_1_relu3-net3"
  type: "ReLU"
  bottom: "layer_128_1_conv2-net3"
  top: "layer_128_1_conv2-net3"
}
layer {
  name: "layer_128_1_conv3-net3"
  type: "Convolution"
  bottom: "layer_128_1_conv2-net3"
  top: "layer_128_1_conv3-net3"
  param {
    name: "weight-128_1_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_conv_expand-net3"
  type: "Convolution"
  bottom: "conv3-add-net2"
  top: "layer_128_1_conv_expand-net3"
  param {
    name: "weight-128_1_conv_expand"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_sum-net3"
  type: "Eltwise"
  bottom: "layer_128_1_conv3-net3"
  bottom: "layer_128_1_conv_expand-net3"
  top: "layer_128_1_sum-net3"
}

layer {
  name: "layer_128_2_bn1-net3"
  type: "BatchNorm"
  bottom: "layer_128_1_sum-net3"
  top: "layer_128_2_bn1-net3"
  param {
    name: "para1-128_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_2_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_2_scale1-net3"
  type: "Scale"
  bottom: "layer_128_2_bn1-net3"
  top: "layer_128_2_bn1-net3"
  param {
    name: "lamda-128_2_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_2_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_2_relu1-net3"
  type: "ReLU"
  bottom: "layer_128_2_bn1-net3"
  top: "layer_128_2_bn1-net3"
}
layer {
  name: "layer_128_2_conv1-net3"
  type: "Convolution"
  bottom: "layer_128_2_bn1-net3"
  top: "layer_128_2_conv1-net3"
  param {
    name: "weight-128_2_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_2_bn2-net3"
  type: "BatchNorm"
  bottom: "layer_128_2_conv1-net3"
  top: "layer_128_2_conv1-net3"
  param {
    name: "para1-128_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_2_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_2_scale2-net3"
  type: "Scale"
  bottom: "layer_128_2_conv1-net3"
  top: "layer_128_2_conv1-net3"
  param {
    name: "lamda-128_2_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_2_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_2_relu2-net3"
  type: "ReLU"
  bottom: "layer_128_2_conv1-net3"
  top: "layer_128_2_conv1-net3"
}
layer {
  name: "layer_128_2_conv2-net3"
  type: "Convolution"
  bottom: "layer_128_2_conv1-net3"
  top: "layer_128_2_conv2-net3"
  param {
    name: "weight-128_2_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_2_bn3-net3"
  type: "BatchNorm"
  bottom: "layer_128_2_conv2-net3"
  top: "layer_128_2_conv2-net3"
  param {
    name: "para1-128_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_2_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_2_scale3-net3"
  type: "Scale"
  bottom: "layer_128_2_conv2-net3"
  top: "layer_128_2_conv2-net3"
  param {
    name: "lamda-128_2_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_2_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_2_relu3-net3"
  type: "ReLU"
  bottom: "layer_128_2_conv2-net3"
  top: "layer_128_2_conv2-net3"
}
layer {
  name: "layer_128_2_conv3-net3"
  type: "Convolution"
  bottom: "layer_128_2_conv2-net3"
  top: "layer_128_2_conv3-net3"
  param {
    name: "weight-128_2_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_2_sum-net3"
  type: "Eltwise"
  bottom: "layer_128_2_conv3-net3"
  bottom: "layer_128_1_sum-net3"
  top: "layer_128_2_sum-net3"
}
layer {
  name: "layer_128_3_bn1-net3"
  type: "BatchNorm"
  bottom: "layer_128_2_sum-net3"
  top: "layer_128_3_bn1-net3"
  param {
    name: "para1-128_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_3_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_3_scale1-net3"
  type: "Scale"
  bottom: "layer_128_3_bn1-net3"
  top: "layer_128_3_bn1-net3"
  param {
    name: "lamda-128_3_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_3_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_3_relu1-net3"
  type: "ReLU"
  bottom: "layer_128_3_bn1-net3"
  top: "layer_128_3_bn1-net3"
}
layer {
  name: "layer_128_3_conv1-net3"
  type: "Convolution"
  bottom: "layer_128_3_bn1-net3"
  top: "layer_128_3_conv1-net3"
  param {
    name: "weight-128_3_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_3_bn2-net3"
  type: "BatchNorm"
  bottom: "layer_128_3_conv1-net3"
  top: "layer_128_3_conv1-net3"
  param {
    name: "para1-128_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_3_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_3_scale2-net3"
  type: "Scale"
  bottom: "layer_128_3_conv1-net3"
  top: "layer_128_3_conv1-net3"
  param {
    name: "lamda-128_3_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_3_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_3_relu2-net3"
  type: "ReLU"
  bottom: "layer_128_3_conv1-net3"
  top: "layer_128_3_conv1-net3"
}
layer {
  name: "layer_128_3_conv2-net3"
  type: "Convolution"
  bottom: "layer_128_3_conv1-net3"
  top: "layer_128_3_conv2-net3"
  param {
    name: "weight-128_3_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_3_bn3-net3"
  type: "BatchNorm"
  bottom: "layer_128_3_conv2-net3"
  top: "layer_128_3_conv2-net3"
  param {
    name: "para1-128_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_3_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_3_scale3-net3"
  type: "Scale"
  bottom: "layer_128_3_conv2-net3"
  top: "layer_128_3_conv2-net3"
  param {
    name: "lamda-128_3_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_3_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_3_relu3-net3"
  type: "ReLU"
  bottom: "layer_128_3_conv2-net3"
  top: "layer_128_3_conv2-net3"
}
layer {
  name: "layer_128_3_conv3-net3"
  type: "Convolution"
  bottom: "layer_128_3_conv2-net3"
  top: "layer_128_3_conv3-net3"
  param {
    name: "weight-128_3_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_3_sum-net3"
  type: "Eltwise"
  bottom: "layer_128_3_conv3-net3"
  bottom: "layer_128_2_sum-net3"
  top: "layer_128_3_sum-net3"
}
layer {
  name: "layer_128_4_bn1-net3"
  type: "BatchNorm"
  bottom: "layer_128_3_sum-net3"
  top: "layer_128_4_bn1-net3"
  param {
    name: "para1-128_4_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_4_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_4_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_4_scale1-net3"
  type: "Scale"
  bottom: "layer_128_4_bn1-net3"
  top: "layer_128_4_bn1-net3"
  param {
    name: "lamda-128_4_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_4_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_4_relu1-net3"
  type: "ReLU"
  bottom: "layer_128_4_bn1-net3"
  top: "layer_128_4_bn1-net3"
}
layer {
  name: "layer_128_4_conv1-net3"
  type: "Convolution"
  bottom: "layer_128_4_bn1-net3"
  top: "layer_128_4_conv1-net3"
  param {
    name: "weight-128_4_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_4_bn2-net3"
  type: "BatchNorm"
  bottom: "layer_128_4_conv1-net3"
  top: "layer_128_4_conv1-net3"
  param {
    name: "para1-128_4_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_4_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_4_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_4_scale2-net3"
  type: "Scale"
  bottom: "layer_128_4_conv1-net3"
  top: "layer_128_4_conv1-net3"
  param {
    name: "lamda-128_4_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_4_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_4_relu2-net3"
  type: "ReLU"
  bottom: "layer_128_4_conv1-net3"
  top: "layer_128_4_conv1-net3"
}
layer {
  name: "layer_128_4_conv2-net3"
  type: "Convolution"
  bottom: "layer_128_4_conv1-net3"
  top: "layer_128_4_conv2-net3"
  param {
    name: "weight-128_4_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_4_bn3-net3"
  type: "BatchNorm"
  bottom: "layer_128_4_conv2-net3"
  top: "layer_128_4_conv2-net3"
  param {
    name: "para1-128_4_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-128_4_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-128_4_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_128_4_scale3-net3"
  type: "Scale"
  bottom: "layer_128_4_conv2-net3"
  top: "layer_128_4_conv2-net3"
  param {
    name: "lamda-128_4_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-128_4_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_4_relu3-net3"
  type: "ReLU"
  bottom: "layer_128_4_conv2-net3"
  top: "layer_128_4_conv2-net3"
}
layer {
  name: "layer_128_4_conv3-net3"
  type: "Convolution"
  bottom: "layer_128_4_conv2-net3"
  top: "layer_128_4_conv3-net3"
  param {
    name: "weight-128_4_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_4_sum-net3"
  type: "Eltwise"
  bottom: "layer_128_4_conv3-net3"
  bottom: "layer_128_3_sum-net3"
  top: "layer_128_4_sum-net3"
}
layer {
  name: "layer_256_1_bn1-net3"
  type: "BatchNorm"
  bottom: "layer_128_4_sum-net3"
  top: "layer_256_1_bn1-net3"
  param {
    name: "para1-256_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_1_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_1_scale1-net3"
  type: "Scale"
  bottom: "layer_256_1_bn1-net3"
  top: "layer_256_1_bn1-net3"
  param {
    name: "lamda-256_1_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_1_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_1_relu1-net3"
  type: "ReLU"
  bottom: "layer_256_1_bn1-net3"
  top: "layer_256_1_bn1-net3"
}
layer {
  name: "layer_512_1_bn1-c-net2"
  type: "Convolution"
  bottom: "layer_512_1_bn1-net2"
  top: "layer_512_1_bn1-c-net2"
  param {
    name: "weight-layer_512_1_bn1-c"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_bn1-c-interp-net2"
  type: "Interp"
  bottom: "layer_512_1_bn1-c-net2"
  top: "layer_512_1_bn1-c-interp-net2"
  interp_param {
    height: 48
    width: 48
  }
}
layer {
  name: "conv4-add-net2"
  type: "Eltwise"
  bottom: "layer_256_1_bn1-net3"
  bottom: "layer_512_1_bn1-c-interp-net2"
  top: "conv4-add-net2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "layer_256_1_conv1-net3"
  type: "Convolution"
  bottom: "conv4-add-net2"
  top: "layer_256_1_conv1-net3"
  param {
    name: "weight-256_1_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_bn2-net3"
  type: "BatchNorm"
  bottom: "layer_256_1_conv1-net3"
  top: "layer_256_1_conv1-net3"
  param {
    name: "para1-256_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_1_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_1_scale2-net3"
  type: "Scale"
  bottom: "layer_256_1_conv1-net3"
  top: "layer_256_1_conv1-net3"
  param {
    name: "lamda-256_1_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_1_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_1_relu2-net3"
  type: "ReLU"
  bottom: "layer_256_1_conv1-net3"
  top: "layer_256_1_conv1-net3"
}
layer {
  name: "layer_256_1_conv2-net3"
  type: "Convolution"
  bottom: "layer_256_1_conv1-net3"
  top: "layer_256_1_conv2-net3"
  param {
    name: "weight-256_1_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_bn3-net3"
  type: "BatchNorm"
  bottom: "layer_256_1_conv2-net3"
  top: "layer_256_1_conv2-net3"
  param {
    name: "para1-256_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_1_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_1_scale3-net3"
  type: "Scale"
  bottom: "layer_256_1_conv2-net3"
  top: "layer_256_1_conv2-net3"
  param {
    name: "lamda-256_1_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_1_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_1_relu3-net3"
  type: "ReLU"
  bottom: "layer_256_1_conv2-net3"
  top: "layer_256_1_conv2-net3"
}
layer {
  name: "layer_256_1_conv3-net3"
  type: "Convolution"
  bottom: "layer_256_1_conv2-net3"
  top: "layer_256_1_conv3-net3"
  param {
    name: "weight-256_1_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_conv_expand-net3"
  type: "Convolution"
  bottom: "conv4-add-net2"
  top: "layer_256_1_conv_expand-net3"
  param {
    name: "weight-256_1_conv_expand"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_sum-net3"
  type: "Eltwise"
  bottom: "layer_256_1_conv3-net3"
  bottom: "layer_256_1_conv_expand-net3"
  top: "layer_256_1_sum-net3"
}

layer {
  name: "layer_256_2_bn1-net3"
  type: "BatchNorm"
  bottom: "layer_256_1_sum-net3"
  top: "layer_256_2_bn1-net3"
  param {
    name: "para1-256_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_2_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_2_scale1-net3"
  type: "Scale"
  bottom: "layer_256_2_bn1-net3"
  top: "layer_256_2_bn1-net3"
  param {
    name: "lamda-256_2_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_2_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_2_relu1-net3"
  type: "ReLU"
  bottom: "layer_256_2_bn1-net3"
  top: "layer_256_2_bn1-net3"
}
layer {
  name: "layer_256_2_conv1-net3"
  type: "Convolution"
  bottom: "layer_256_2_bn1-net3"
  top: "layer_256_2_conv1-net3"
  param {
    name: "weight-256_2_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_2_bn2-net3"
  type: "BatchNorm"
  bottom: "layer_256_2_conv1-net3"
  top: "layer_256_2_conv1-net3"
  param {
    name: "para1-256_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_2_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_2_scale2-net3"
  type: "Scale"
  bottom: "layer_256_2_conv1-net3"
  top: "layer_256_2_conv1-net3"
  param {
    name: "lamda-256_2_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_2_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_2_relu2-net3"
  type: "ReLU"
  bottom: "layer_256_2_conv1-net3"
  top: "layer_256_2_conv1-net3"
}
layer {
  name: "layer_256_2_conv2-net3"
  type: "Convolution"
  bottom: "layer_256_2_conv1-net3"
  top: "layer_256_2_conv2-net3"
  param {
    name: "weight-256_2_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_2_bn3-net3"
  type: "BatchNorm"
  bottom: "layer_256_2_conv2-net3"
  top: "layer_256_2_conv2-net3"
  param {
    name: "para1-256_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_2_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_2_scale3-net3"
  type: "Scale"
  bottom: "layer_256_2_conv2-net3"
  top: "layer_256_2_conv2-net3"
  param {
    name: "lamda-256_2_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_2_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_2_relu3-net3"
  type: "ReLU"
  bottom: "layer_256_2_conv2-net3"
  top: "layer_256_2_conv2-net3"
}
layer {
  name: "layer_256_2_conv3-net3"
  type: "Convolution"
  bottom: "layer_256_2_conv2-net3"
  top: "layer_256_2_conv3-net3"
  param {
    name: "weight-256_2_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_2_sum-net3"
  type: "Eltwise"
  bottom: "layer_256_2_conv3-net3"
  bottom: "layer_256_1_sum-net3"
  top: "layer_256_2_sum-net3"
}
layer {
  name: "layer_256_3_bn1-net3"
  type: "BatchNorm"
  bottom: "layer_256_2_sum-net3"
  top: "layer_256_3_bn1-net3"
  param {
    name: "para1-256_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_3_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_3_scale1-net3"
  type: "Scale"
  bottom: "layer_256_3_bn1-net3"
  top: "layer_256_3_bn1-net3"
  param {
    name: "lamda-256_3_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_3_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_3_relu1-net3"
  type: "ReLU"
  bottom: "layer_256_3_bn1-net3"
  top: "layer_256_3_bn1-net3"
}
layer {
  name: "layer_256_3_conv1-net3"
  type: "Convolution"
  bottom: "layer_256_3_bn1-net3"
  top: "layer_256_3_conv1-net3"
  param {
    name: "weight-256_3_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_3_bn2-net3"
  type: "BatchNorm"
  bottom: "layer_256_3_conv1-net3"
  top: "layer_256_3_conv1-net3"
  param {
    name: "para1-256_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_3_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_3_scale2-net3"
  type: "Scale"
  bottom: "layer_256_3_conv1-net3"
  top: "layer_256_3_conv1-net3"
  param {
    name: "lamda-256_3_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_3_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_3_relu2-net3"
  type: "ReLU"
  bottom: "layer_256_3_conv1-net3"
  top: "layer_256_3_conv1-net3"
}
layer {
  name: "layer_256_3_conv2-net3"
  type: "Convolution"
  bottom: "layer_256_3_conv1-net3"
  top: "layer_256_3_conv2-net3"
  param {
    name: "weight-256_3_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_3_bn3-net3"
  type: "BatchNorm"
  bottom: "layer_256_3_conv2-net3"
  top: "layer_256_3_conv2-net3"
  param {
    name: "para1-256_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_3_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_3_scale3-net3"
  type: "Scale"
  bottom: "layer_256_3_conv2-net3"
  top: "layer_256_3_conv2-net3"
  param {
    name: "lamda-256_3_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_3_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_3_relu3-net3"
  type: "ReLU"
  bottom: "layer_256_3_conv2-net3"
  top: "layer_256_3_conv2-net3"
}
layer {
  name: "layer_256_3_conv3-net3"
  type: "Convolution"
  bottom: "layer_256_3_conv2-net3"
  top: "layer_256_3_conv3-net3"
  param {
    name: "weight-256_3_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_3_sum-net3"
  type: "Eltwise"
  bottom: "layer_256_3_conv3-net3"
  bottom: "layer_256_2_sum-net3"
  top: "layer_256_3_sum-net3"
}
layer {
  name: "layer_256_4_bn1-net3"
  type: "BatchNorm"
  bottom: "layer_256_3_sum-net3"
  top: "layer_256_4_bn1-net3"
  param {
    name: "para1-256_4_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_4_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_4_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_4_scale1-net3"
  type: "Scale"
  bottom: "layer_256_4_bn1-net3"
  top: "layer_256_4_bn1-net3"
  param {
    name: "lamda-256_4_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_4_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_4_relu1-net3"
  type: "ReLU"
  bottom: "layer_256_4_bn1-net3"
  top: "layer_256_4_bn1-net3"
}
layer {
  name: "layer_256_4_conv1-net3"
  type: "Convolution"
  bottom: "layer_256_4_bn1-net3"
  top: "layer_256_4_conv1-net3"
  param {
    name: "weight-256_4_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_4_bn2-net3"
  type: "BatchNorm"
  bottom: "layer_256_4_conv1-net3"
  top: "layer_256_4_conv1-net3"
  param {
    name: "para1-256_4_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_4_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_4_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_4_scale2-net3"
  type: "Scale"
  bottom: "layer_256_4_conv1-net3"
  top: "layer_256_4_conv1-net3"
  param {
    name: "lamda-256_4_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_4_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_4_relu2-net3"
  type: "ReLU"
  bottom: "layer_256_4_conv1-net3"
  top: "layer_256_4_conv1-net3"
}
layer {
  name: "layer_256_4_conv2-net3"
  type: "Convolution"
  bottom: "layer_256_4_conv1-net3"
  top: "layer_256_4_conv2-net3"
  param {
    name: "weight-256_4_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_4_bn3-net3"
  type: "BatchNorm"
  bottom: "layer_256_4_conv2-net3"
  top: "layer_256_4_conv2-net3"
  param {
    name: "para1-256_4_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_4_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_4_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_4_scale3-net3"
  type: "Scale"
  bottom: "layer_256_4_conv2-net3"
  top: "layer_256_4_conv2-net3"
  param {
    name: "lamda-256_4_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_4_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_4_relu3-net3"
  type: "ReLU"
  bottom: "layer_256_4_conv2-net3"
  top: "layer_256_4_conv2-net3"
}
layer {
  name: "layer_256_4_conv3-net3"
  type: "Convolution"
  bottom: "layer_256_4_conv2-net3"
  top: "layer_256_4_conv3-net3"
  param {
    name: "weight-256_4_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_4_sum-net3"
  type: "Eltwise"
  bottom: "layer_256_4_conv3-net3"
  bottom: "layer_256_3_sum-net3"
  top: "layer_256_4_sum-net3"
}
layer {
  name: "layer_256_5_bn1-net3"
  type: "BatchNorm"
  bottom: "layer_256_4_sum-net3"
  top: "layer_256_5_bn1-net3"
  param {
    name: "para1-256_5_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_5_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_5_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_5_scale1-net3"
  type: "Scale"
  bottom: "layer_256_5_bn1-net3"
  top: "layer_256_5_bn1-net3"
  param {
    name: "lamda-256_5_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_5_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_5_relu1-net3"
  type: "ReLU"
  bottom: "layer_256_5_bn1-net3"
  top: "layer_256_5_bn1-net3"
}
layer {
  name: "layer_256_5_conv1-net3"
  type: "Convolution"
  bottom: "layer_256_5_bn1-net3"
  top: "layer_256_5_conv1-net3"
  param {
    name: "weight-256_5_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_5_bn2-net3"
  type: "BatchNorm"
  bottom: "layer_256_5_conv1-net3"
  top: "layer_256_5_conv1-net3"
  param {
    name: "para1-256_5_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_5_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_5_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_5_scale2-net3"
  type: "Scale"
  bottom: "layer_256_5_conv1-net3"
  top: "layer_256_5_conv1-net3"
  param {
    name: "lamda-256_5_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_5_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_5_relu2-net3"
  type: "ReLU"
  bottom: "layer_256_5_conv1-net3"
  top: "layer_256_5_conv1-net3"
}
layer {
  name: "layer_256_5_conv2-net3"
  type: "Convolution"
  bottom: "layer_256_5_conv1-net3"
  top: "layer_256_5_conv2-net3"
  param {
    name: "weight-256_5_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_5_bn3-net3"
  type: "BatchNorm"
  bottom: "layer_256_5_conv2-net3"
  top: "layer_256_5_conv2-net3"
  param {
    name: "para1-256_5_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_5_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_5_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_5_scale3-net3"
  type: "Scale"
  bottom: "layer_256_5_conv2-net3"
  top: "layer_256_5_conv2-net3"
  param {
    name: "lamda-256_5_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_5_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_5_relu3-net3"
  type: "ReLU"
  bottom: "layer_256_5_conv2-net3"
  top: "layer_256_5_conv2-net3"
}
layer {
  name: "layer_256_5_conv3-net3"
  type: "Convolution"
  bottom: "layer_256_5_conv2-net3"
  top: "layer_256_5_conv3-net3"
  param {
    name: "weight-256_5_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_5_sum-net3"
  type: "Eltwise"
  bottom: "layer_256_5_conv3-net3"
  bottom: "layer_256_4_sum-net3"
  top: "layer_256_5_sum-net3"
}
layer {
  name: "layer_256_6_bn1-net3"
  type: "BatchNorm"
  bottom: "layer_256_5_sum-net3"
  top: "layer_256_6_bn1-net3"
  param {
    name: "para1-256_6_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_6_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_6_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_6_scale1-net3"
  type: "Scale"
  bottom: "layer_256_6_bn1-net3"
  top: "layer_256_6_bn1-net3"
  param {
    name: "lamda-256_6_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_6_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_6_relu1-net3"
  type: "ReLU"
  bottom: "layer_256_6_bn1-net3"
  top: "layer_256_6_bn1-net3"
}
layer {
  name: "layer_256_6_conv1-net3"
  type: "Convolution"
  bottom: "layer_256_6_bn1-net3"
  top: "layer_256_6_conv1-net3"
  param {
    name: "weight-256_6_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_6_bn2-net3"
  type: "BatchNorm"
  bottom: "layer_256_6_conv1-net3"
  top: "layer_256_6_conv1-net3"
  param {
    name: "para1-256_6_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_6_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_6_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_6_scale2-net3"
  type: "Scale"
  bottom: "layer_256_6_conv1-net3"
  top: "layer_256_6_conv1-net3"
  param {
    name: "lamda-256_6_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_6_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_6_relu2-net3"
  type: "ReLU"
  bottom: "layer_256_6_conv1-net3"
  top: "layer_256_6_conv1-net3"
}
layer {
  name: "layer_256_6_conv2-net3"
  type: "Convolution"
  bottom: "layer_256_6_conv1-net3"
  top: "layer_256_6_conv2-net3"
  param {
    name: "weight-256_6_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_6_bn3-net3"
  type: "BatchNorm"
  bottom: "layer_256_6_conv2-net3"
  top: "layer_256_6_conv2-net3"
  param {
    name: "para1-256_6_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-256_6_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-256_6_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_256_6_scale3-net3"
  type: "Scale"
  bottom: "layer_256_6_conv2-net3"
  top: "layer_256_6_conv2-net3"
  param {
    name: "lamda-256_6_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-256_6_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_6_relu3-net3"
  type: "ReLU"
  bottom: "layer_256_6_conv2-net3"
  top: "layer_256_6_conv2-net3"
}
layer {
  name: "layer_256_6_conv3-net3"
  type: "Convolution"
  bottom: "layer_256_6_conv2-net3"
  top: "layer_256_6_conv3-net3"
  param {
    name: "weight-256_6_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_6_sum-net3"
  type: "Eltwise"
  bottom: "layer_256_6_conv3-net3"
  bottom: "layer_256_5_sum-net3"
  top: "layer_256_6_sum-net3"
}
layer {
  name: "layer_512_1_bn1-net3"
  type: "BatchNorm"
  bottom: "layer_256_6_sum-net3"
  top: "layer_512_1_bn1-net3"
  param {
    name: "para1-512_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_1_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_1_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_1_scale1-net3"
  type: "Scale"
  bottom: "layer_512_1_bn1-net3"
  top: "layer_512_1_bn1-net3"
  param {
    name: "lamda-512_1_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_1_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_1_relu1-net3"
  type: "ReLU"
  bottom: "layer_512_1_bn1-net3"
  top: "layer_512_1_bn1-net3"
}

layer {
  name: "layer_512_3_sum-c-net2"
  type: "Convolution"
  bottom: "layer_512_3_sum-net2"
  top: "layer_512_3_sum-c-net2"
  param {
    name: "weight-layer_512_3_sum-c"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_3_sum-c-interp-net2"
  type: "Interp"
  bottom: "layer_512_3_sum-c-net2"
  top: "layer_512_3_sum-c-interp-net2"
  interp_param {
    height: 24
    width: 24
  }
}
layer {
  name: "conv5-add-net2"
  type: "Eltwise"
  bottom: "layer_512_1_bn1-net3"
  bottom: "layer_512_3_sum-c-interp-net2"
  top: "conv5-add-net2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "layer_512_1_conv1-net3"
  type: "Convolution"
  bottom: "conv5-add-net2"
  top: "layer_512_1_conv1-net3"
  param {
    name: "weight-512_1_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_bn2-net3"
  type: "BatchNorm"
  bottom: "layer_512_1_conv1-net3"
  top: "layer_512_1_conv1-net3"
  param {
    name: "para1-512_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_1_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_1_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_1_scale2-net3"
  type: "Scale"
  bottom: "layer_512_1_conv1-net3"
  top: "layer_512_1_conv1-net3"
  param {
    name: "lamda-512_1_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_1_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_1_relu2-net3"
  type: "ReLU"
  bottom: "layer_512_1_conv1-net3"
  top: "layer_512_1_conv1-net3"
}
layer {
  name: "layer_512_1_conv2-net3"
  type: "Convolution"
  bottom: "layer_512_1_conv1-net3"
  top: "layer_512_1_conv2-net3"
  param {
    name: "weight-512_1_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_bn3-net3"
  type: "BatchNorm"
  bottom: "layer_512_1_conv2-net3"
  top: "layer_512_1_conv2-net3"
  param {
    name: "para1-512_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_1_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_1_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_1_scale3-net3"
  type: "Scale"
  bottom: "layer_512_1_conv2-net3"
  top: "layer_512_1_conv2-net3"
  param {
    name: "lamda-512_1_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_1_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_1_relu3-net3"
  type: "ReLU"
  bottom: "layer_512_1_conv2-net3"
  top: "layer_512_1_conv2-net3"
}
layer {
  name: "layer_512_1_conv3-net3"
  type: "Convolution"
  bottom: "layer_512_1_conv2-net3"
  top: "layer_512_1_conv3-net3"
  param {
    name: "weight-512_1_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 2048
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_conv_expand-net3"
  type: "Convolution"
  bottom: "conv5-add-net2"
  top: "layer_512_1_conv_expand-net3"
  param {
    name: "weight-512_1_conv_expand"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 2048
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_sum-net3"
  type: "Eltwise"
  bottom: "layer_512_1_conv3-net3"
  bottom: "layer_512_1_conv_expand-net3"
  top: "layer_512_1_sum-net3"
}

layer {
  name: "layer_512_2_bn1-net3"
  type: "BatchNorm"
  bottom: "layer_512_1_sum-net3"
  top: "layer_512_2_bn1-net3"
  param {
    name: "para1-512_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_2_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_2_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_2_scale1-net3"
  type: "Scale"
  bottom: "layer_512_2_bn1-net3"
  top: "layer_512_2_bn1-net3"
  param {
    name: "lamda-512_2_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_2_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_2_relu1-net3"
  type: "ReLU"
  bottom: "layer_512_2_bn1-net3"
  top: "layer_512_2_bn1-net3"
}
layer {
  name: "layer_512_2_conv1-net3"
  type: "Convolution"
  bottom: "layer_512_2_bn1-net3"
  top: "layer_512_2_conv1-net3"
  param {
    name: "weight-512_2_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_2_bn2-net3"
  type: "BatchNorm"
  bottom: "layer_512_2_conv1-net3"
  top: "layer_512_2_conv1-net3"
  param {
    name: "para1-512_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_2_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_2_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_2_scale2-net3"
  type: "Scale"
  bottom: "layer_512_2_conv1-net3"
  top: "layer_512_2_conv1-net3"
  param {
    name: "lamda-512_2_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_2_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_2_relu2-net3"
  type: "ReLU"
  bottom: "layer_512_2_conv1-net3"
  top: "layer_512_2_conv1-net3"
}
layer {
  name: "layer_512_2_conv2-net3"
  type: "Convolution"
  bottom: "layer_512_2_conv1-net3"
  top: "layer_512_2_conv2-net3"
  param {
    name: "weight-512_2_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_2_bn3-net3"
  type: "BatchNorm"
  bottom: "layer_512_2_conv2-net3"
  top: "layer_512_2_conv2-net3"
  param {
    name: "para1-512_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_2_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_2_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_2_scale3-net3"
  type: "Scale"
  bottom: "layer_512_2_conv2-net3"
  top: "layer_512_2_conv2-net3"
  param {
    name: "lamda-512_2_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_2_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_2_relu3-net3"
  type: "ReLU"
  bottom: "layer_512_2_conv2-net3"
  top: "layer_512_2_conv2-net3"
}
layer {
  name: "layer_512_2_conv3-net3"
  type: "Convolution"
  bottom: "layer_512_2_conv2-net3"
  top: "layer_512_2_conv3-net3"
  param {
    name: "weight-512_2_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 2048
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_2_sum-net3"
  type: "Eltwise"
  bottom: "layer_512_2_conv3-net3"
  bottom: "layer_512_1_sum-net3"
  top: "layer_512_2_sum-net3"
}
layer {
  name: "layer_512_3_bn1-net3"
  type: "BatchNorm"
  bottom: "layer_512_2_sum-net3"
  top: "layer_512_3_bn1-net3"
  param {
    name: "para1-512_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_3_bn1"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_3_bn1"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_3_scale1-net3"
  type: "Scale"
  bottom: "layer_512_3_bn1-net3"
  top: "layer_512_3_bn1-net3"
  param {
    name: "lamda-512_3_scale1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_3_scale1"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_3_relu1-net3"
  type: "ReLU"
  bottom: "layer_512_3_bn1-net3"
  top: "layer_512_3_bn1-net3"
}
layer {
  name: "layer_512_3_conv1-net3"
  type: "Convolution"
  bottom: "layer_512_3_bn1-net3"
  top: "layer_512_3_conv1-net3"
  param {
    name: "weight-512_3_conv1"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_3_bn2-net3"
  type: "BatchNorm"
  bottom: "layer_512_3_conv1-net3"
  top: "layer_512_3_conv1-net3"
  param {
    name: "para1-512_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_3_bn2"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_3_bn2"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_3_scale2-net3"
  type: "Scale"
  bottom: "layer_512_3_conv1-net3"
  top: "layer_512_3_conv1-net3"
  param {
    name: "lamda-512_3_scale2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_3_scale2"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_3_relu2-net3"
  type: "ReLU"
  bottom: "layer_512_3_conv1-net3"
  top: "layer_512_3_conv1-net3"
}
layer {
  name: "layer_512_3_conv2-net3"
  type: "Convolution"
  bottom: "layer_512_3_conv1-net3"
  top: "layer_512_3_conv2-net3"
  param {
    name: "weight-512_3_conv2"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_3_bn3-net3"
  type: "BatchNorm"
  bottom: "layer_512_3_conv2-net3"
  top: "layer_512_3_conv2-net3"
  param {
    name: "para1-512_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para2-512_3_bn3"
    lr_mult: 0.0
  }
  param {
    name: "para3-512_3_bn3"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "layer_512_3_scale3-net3"
  type: "Scale"
  bottom: "layer_512_3_conv2-net3"
  top: "layer_512_3_conv2-net3"
  param {
    name: "lamda-512_3_scale3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-512_3_scale3"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_3_relu3-net3"
  type: "ReLU"
  bottom: "layer_512_3_conv2-net3"
  top: "layer_512_3_conv2-net3"
}
layer {
  name: "layer_512_3_conv3-net3"
  type: "Convolution"
  bottom: "layer_512_3_conv2-net3"
  top: "layer_512_3_conv3-net3"
  param {
    name: "weight-512_3_conv3"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 2048
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_3_sum-net3"
  type: "Eltwise"
  bottom: "layer_512_3_conv3-net3"
  bottom: "layer_512_2_sum-net3"
  top: "layer_512_3_sum-net3"
}
layer {
  name: "last_bn-net3"
  type: "BatchNorm"
  bottom: "layer_512_3_sum-net3"
  top: "layer_512_3_sum-net3"
  param {
    name: "para1-last_bn"
    lr_mult: 0.0
  }
  param {
    name: "para2-last_bn"
    lr_mult: 0.0
  }
  param {
    name: "para3-last_bn"
    lr_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "last_scale-net3"
  type: "Scale"
  bottom: "layer_512_3_sum-net3"
  top: "layer_512_3_sum-net3"
  param {
    name: "lamda-last_scale"
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    name: "beta-last_scale"
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "last_relu-net3"
  type: "ReLU"
  bottom: "layer_512_3_sum-net3"
  top: "layer_512_3_sum-net3"
}
#
# conv5 output
#
layer {
  name: "conv6-net3"
  type: "Convolution"
  bottom: "layer_512_3_sum-net3"
  top: "conv6-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
# 
# spatial attention
#
layer {
  name: "conv5_4/bn-pool1-net3"
  type: "Interp"
  bottom: "layer_512_3_sum-net3"
  top: "conv5_4/bn-pool1-net3"
  interp_param {
    height: 6
    width: 6
  }
}

layer {
  name: "spa.att_Conv_1-net3"
  type: "Convolution"
  bottom: "conv5_4/bn-pool1-net3"
  top: "spa.att_Conv_1-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_1-norm-net3"
  type: "Normalize"
  bottom: "spa.att_Conv_1-net3"
  top: "spa.att_Conv_1-norm-net3"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_2-net3"
  type: "Convolution"
  bottom: "conv5_4/bn-pool1-net3"
  top: "spa.att_Conv_2-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_2-norm-net3"
  type: "Normalize"
  bottom: "spa.att_Conv_2-net3"
  top: "spa.att_Conv_2-norm-net3"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_3-net3"
  type: "Convolution"
  bottom: "conv5_4/bn-pool1-net3"
  top: "spa.att_Conv_3-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_3-norm-net3"
  type: "Normalize"
  bottom: "spa.att_Conv_3-net3"
  top: "spa.att_Conv_3-norm-net3"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_all-norm-net3"
  type: "Concat"
  bottom: "spa.att_Conv_1-norm-net3"
  bottom: "spa.att_Conv_2-norm-net3"
  bottom: "spa.att_Conv_3-norm-net3"
  top: "spa.att_all-norm-net3"
}
layer {
  name: "spa.att_all-norm_mask-net3"
  type: "Convolution"
  bottom: "spa.att_all-norm-net3"
  top: "spa.att_all-norm_mask-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 1
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Reshape_4-net3"
  type: "Reshape"
  bottom: "spa.att_all-norm_mask-net3"
  top: "spa.att_Reshape_4-net3"
  reshape_param {
    shape {
      dim: -1
      dim: 36
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "spa.att_softmax_5-net3"
  type: "Softmax"
  bottom: "spa.att_Reshape_4-net3"
  top: "spa.att_softmax_5-net3"
  softmax_param {
    axis: 1
  }
}
layer {
  name: "spa.att_Reshape_6-net3"
  type: "Reshape"
  bottom: "spa.att_softmax_5-net3"
  top: "spa.att_Reshape_6-net3"
  reshape_param {
    shape {
      dim: -1
      dim: 1
      dim: 6
      dim: 6
    }
  }
}
layer {
  name: "spa.att_Reshape_6-power-net3"
  type: "Power"
  bottom: "spa.att_Reshape_6-net3"
  top: "spa.att_Reshape_6-power-net3"
  power_param {
    power: 1
    scale: 36
    shift: 0
  }
}
layer {
  name: "spa.att_Reshape_6-power_interp-net3"
  type: "Interp"
  bottom: "spa.att_Reshape_6-power-net3"
  top: "spa.att_Reshape_6-power_interp-net3"
  interp_param {
    height: 12
    width: 12
  }
}
layer {
  name: "spa.att_Reshape_6-power_tile-net3"
  type: "Tile"
  bottom: "spa.att_Reshape_6-power_interp-net3"
  top: "spa.att_Reshape_6-power_tile-net3"
  tile_param {
    tiles: 128
  }
}
# 
# spatial end
#
layer {
  name: "conv6_prod-net3"
  type: "Eltwise"
  bottom: "conv6-net3"
  bottom: "spa.att_Reshape_6-power_tile-net3"
  top: "conv6_prod-net3"
  eltwise_param {
    operation: PROD
  }
}
#
#
#
# conv4 output
#
layer {
  name: "conv6-c4-net3"
  type: "Convolution"
  bottom: "layer_512_1_bn1-net3"
  top: "conv6-c4-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
# 
# spatial attention
#
layer {
  name: "conv4_4/bn-pool1-c4-net3"
  type: "Interp"
  bottom: "layer_512_1_bn1-net3"
  top: "conv4_4/bn-pool1-c4-net3"
  interp_param {
    height: 12
    width: 12
  }
}

layer {
  name: "spa.att_Conv_1-c4-net3"
  type: "Convolution"
  bottom: "conv4_4/bn-pool1-c4-net3"
  top: "spa.att_Conv_1-c4-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_1-norm-c4-net3"
  type: "Normalize"
  bottom: "spa.att_Conv_1-c4-net3"
  top: "spa.att_Conv_1-norm-c4-net3"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_2-c4-net3"
  type: "Convolution"
  bottom: "conv4_4/bn-pool1-c4-net3"
  top: "spa.att_Conv_2-c4-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_2-norm-c4-net3"
  type: "Normalize"
  bottom: "spa.att_Conv_2-c4-net3"
  top: "spa.att_Conv_2-norm-c4-net3"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_3-c4-net3"
  type: "Convolution"
  bottom: "conv4_4/bn-pool1-c4-net3"
  top: "spa.att_Conv_3-c4-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_3-norm-c4-net3"
  type: "Normalize"
  bottom: "spa.att_Conv_3-c4-net3"
  top: "spa.att_Conv_3-norm-c4-net3"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_all-norm-c4-net3"
  type: "Concat"
  bottom: "spa.att_Conv_1-norm-c4-net3"
  bottom: "spa.att_Conv_2-norm-c4-net3"
  bottom: "spa.att_Conv_3-norm-c4-net3"
  top: "spa.att_all-norm-c4-net3"
}
layer {
  name: "spa.att_all-norm_mask-c4-net3"
  type: "Convolution"
  bottom: "spa.att_all-norm-c4-net3"
  top: "spa.att_all-norm_mask-c4-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 1
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Reshape_4-c4-net3"
  type: "Reshape"
  bottom: "spa.att_all-norm_mask-c4-net3"
  top: "spa.att_Reshape_4-c4-net3"
  reshape_param {
    shape {
      dim: -1
      dim: 144
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "spa.att_softmax_5-c4-net3"
  type: "Softmax"
  bottom: "spa.att_Reshape_4-c4-net3"
  top: "spa.att_softmax_5-c4-net3"
  softmax_param {
    axis: 1
  }
}
layer {
  name: "spa.att_Reshape_6-c4-net3"
  type: "Reshape"
  bottom: "spa.att_softmax_5-c4-net3"
  top: "spa.att_Reshape_6-c4-net3"
  reshape_param {
    shape {
      dim: -1
      dim: 1
      dim: 12
      dim: 12
    }
  }
}
layer {
  name: "spa.att_Reshape_6-power-c4-net3"
  type: "Power"
  bottom: "spa.att_Reshape_6-c4-net3"
  top: "spa.att_Reshape_6-power-c4-net3"
  power_param {
    power: 1
    scale: 144
    shift: 0
  }
}
layer {
  name: "spa.att_Reshape_6-power_interp-c4-net3"
  type: "Interp"
  bottom: "spa.att_Reshape_6-power-c4-net3"
  top: "spa.att_Reshape_6-power_interp-c4-net3"
  interp_param {
    height: 24
    width: 24
  }
}
layer {
  name: "spa.att_Reshape_6-power_tile-c4-net3"
  type: "Tile"
  bottom: "spa.att_Reshape_6-power_interp-c4-net3"
  top: "spa.att_Reshape_6-power_tile-c4-net3"
  tile_param {
    tiles: 128
  }
}
# 
# spatial end
#
layer {
  name: "conv6_prod-c4-net3"
  type: "Eltwise"
  bottom: "conv6-c4-net3"
  bottom: "spa.att_Reshape_6-power_tile-c4-net3"
  top: "conv6_prod-c4-net3"
  eltwise_param {
    operation: PROD
  }
}


# conv3 output
#
layer {
  name: "conv6-c3-net3"
  type: "Convolution"
  bottom: "layer_256_1_bn1-net3"
  top: "conv6-c3-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
# 
# spatial attention
#
layer {
  name: "conv3_4/bn-pool1-c3-net3"
  type: "Interp"
  bottom: "layer_256_1_bn1-net3"
  top: "conv3_4/bn-pool1-c3-net3"
  interp_param {
    height: 24
    width: 24
  }
}

layer {
  name: "spa.att_Conv_1-c3-net3"
  type: "Convolution"
  bottom: "conv3_4/bn-pool1-c3-net3"
  top: "spa.att_Conv_1-c3-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_1-norm-c3-net3"
  type: "Normalize"
  bottom: "spa.att_Conv_1-c3-net3"
  top: "spa.att_Conv_1-norm-c3-net3"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_2-c3-net3"
  type: "Convolution"
  bottom: "conv3_4/bn-pool1-c3-net3"
  top: "spa.att_Conv_2-c3-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_2-norm-c3-net3"
  type: "Normalize"
  bottom: "spa.att_Conv_2-c3-net3"
  top: "spa.att_Conv_2-norm-c3-net3"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_Conv_3-c3-net3"
  type: "Convolution"
  bottom: "conv3_4/bn-pool1-c3-net3"
  top: "spa.att_Conv_3-c3-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Conv_3-norm-c3-net3"
  type: "Normalize"
  bottom: "spa.att_Conv_3-c3-net3"
  top: "spa.att_Conv_3-norm-c3-net3"
  norm_param {
    scale_filler {
	type: "constant"
	value: 10
    }
    across_spatial: false
    channel_shared: false
    fix_scale: false

  }
}

layer {
  name: "spa.att_all-norm-c3-net3"
  type: "Concat"
  bottom: "spa.att_Conv_1-norm-c3-net3"
  bottom: "spa.att_Conv_2-norm-c3-net3"
  bottom: "spa.att_Conv_3-norm-c3-net3"
  top: "spa.att_all-norm-c3-net3"
}
layer {
  name: "spa.att_all-norm_mask-c3-net3"
  type: "Convolution"
  bottom: "spa.att_all-norm-c3-net3"
  top: "spa.att_all-norm_mask-c3-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  convolution_param {
    num_output: 1
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_term: false
  }
}
layer {
  name: "spa.att_Reshape_4-c3-net3"
  type: "Reshape"
  bottom: "spa.att_all-norm_mask-c3-net3"
  top: "spa.att_Reshape_4-c3-net3"
  reshape_param {
    shape {
      dim: -1
      dim: 576
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "spa.att_softmax_5-c3-net3"
  type: "Softmax"
  bottom: "spa.att_Reshape_4-c3-net3"
  top: "spa.att_softmax_5-c3-net3"
  softmax_param {
    axis: 1
  }
}
layer {
  name: "spa.att_Reshape_6-c3-net3"
  type: "Reshape"
  bottom: "spa.att_softmax_5-c3-net3"
  top: "spa.att_Reshape_6-c3-net3"
  reshape_param {
    shape {
      dim: -1
      dim: 1
      dim: 24
      dim: 24
    }
  }
}
layer {
  name: "spa.att_Reshape_6-power-c3-net3"
  type: "Power"
  bottom: "spa.att_Reshape_6-c3-net3"
  top: "spa.att_Reshape_6-power-c3-net3"
  power_param {
    power: 1
    scale: 576
    shift: 0
  }
}
layer {
  name: "spa.att_Reshape_6-power_interp-c3-net3"
  type: "Interp"
  bottom: "spa.att_Reshape_6-power-c3-net3"
  top: "spa.att_Reshape_6-power_interp-c3-net3"
  interp_param {
    height: 48
    width: 48
  }
}
layer {
  name: "spa.att_Reshape_6-power_tile-c3-net3"
  type: "Tile"
  bottom: "spa.att_Reshape_6-power_interp-c3-net3"
  top: "spa.att_Reshape_6-power_tile-c3-net3"
  tile_param {
    tiles: 128
  }
}
# 
# spatial end
#
layer {
  name: "conv6_prod-c3-net3"
  type: "Eltwise"
  bottom: "conv6-c3-net3"
  bottom: "spa.att_Reshape_6-power_tile-c3-net3"
  top: "conv6_prod-c3-net3"
  eltwise_param {
    operation: PROD
  }
}
#
#
#
#
# three scale concat
#
layer {
  name: "conv6_prod_interp-net3"
  type: "Interp"
  bottom: "conv6_prod-net3"
  top: "conv6_prod_interp-net3"
  interp_param {
    height: 48
    width: 48
  }
}
layer {
  name: "conv6_prod-c4_interp-net3"
  type: "Interp"
  bottom: "conv6_prod-c4-net3"
  top: "conv6_prod-c4_interp-net3"
  interp_param {
    height: 48
    width: 48
  }
}

layer {
  name: "conv6_prod-concat-net3"
  type: "Eltwise"
  bottom: "conv6_prod-c3-net3"
  bottom: "conv6_prod-c4_interp-net3"
  bottom: "conv6_prod_interp-net3"
  top: "conv6_prod-concat-net3"

}
layer {
  name: "conv7-net3"
  type: "Convolution"
  bottom: "conv6_prod-concat-net3"
  top: "conv7-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "conv7-2-net3"
  type: "Convolution"
  bottom: "conv7-net3"
  top: "conv7-2-net3"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  convolution_param {
    num_output: 2
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "conv7_interp-net3"
  type: "Interp"
  bottom: "conv7-2-net3"
  top: "conv7_interp-net3"
  interp_param {
    height: 384
    width: 384
  }
}
layer {
  name: "loss-net3"
  type: "SoftmaxWithLoss"
  bottom: "conv7_interp-net3"
  bottom: "label-net3"
  top: "loss-net3"
  loss_param {
#    ignore_label: 255
    normalize: false
  }
}
#
#
